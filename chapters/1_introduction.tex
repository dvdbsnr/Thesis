% Chapter 1: Introduction
% Contains:
%   An introduction to random graph theory
%   Basic results on the size of random graph components
%   Basic results on the critical window
%   The main theorem:
%      Motivation
%      The Brownian motion
%      Complete statement
%   Overview of coming proof

\chapter{Introduction}
\fxnote{Update title.}

A brief introduction.
\lipsum[100]

\section{The Erdös-Renyi random graph}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: The Erdös-Renyi random graph
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In their paper, Erdös and Renyi proposed a model of random graph that is now known as the Erdös-Renyi random graph.
There are several ways to construct this random graph which we will briefly introduce,
starting with the historic models used in this first paper 
and continuing with the equivalent model which provides the basis for Aldous' paper and consequently this thesis.

We denote by $G_{n,M}$ the set of graphs on $n$ vertices with $M$ edges that are
\begin{enumerate}
	\item undirected,
	\item without slings, i.e. there is no edge $(v,v)$ for some vertex $v$,
	\item without parallel edges, i.e. there can be at most one edge $(v_1, v_2)$ for each pair of vertices.
\end{enumerate}
A graph in $G_{n,M}$ can be constructed by choosing $M$ out of the $\binom{n}{2}$ possible edges between the vertices,
which leads to a total number of graphs
\begin{equation}
	|G_{n,M}| = \binom{\binom{n}{2}}{M}.
\end{equation}
The Erdös-Renyi random graph $\Gcal(n, M)$ is now obtained by choosing one element of $G_{n,M}$ at random
with equal probability for each graph, where the number of edges is usually dependent on the number of vertices, $M = M(n)$.

An equivalent definition provides the following process:
Starting with a graph on $n$ vertices with $0$ edges at time $t=1$, 
pick one of the $\binom{n}{2}$ possible edges at random with equal probability for each edge,
label it $e_1$.
At time $t=2$ pick one of the remaining $\binom{n}{2}-1$ edges, again all remaining edges being equiprobable,
and denote it by $e_2$.
Continue until $M$ edges have been chosen at time $t=M$.
The graph on vertices $\{1, \dots, n\}$ with edges $\{e_1, \dots, e_M\}$ is the desired Erdös-Renyi random graph.

Using these definitions Erdös and Renyi prove that,
when increasing the number of edges $M(n)$,
the sizes of connected components undergo distinct phases leading from small sparse components
to one giant component and eventually a completely connected graph.

Most current literature and the paper we are studying uses a slightly different approach to constructing the Erdös-Renyi random graph.
Given a set of vertices $\{1, \dots, n\}$ and an edge-probability $\p = p(n)$
we take every edge $(v_1, v_2)$ and add it to the set of edges of the random graph with probability $\p$ independently.
We denote a random object constructed this way by $\Gcal(n, \p)$ and expect a realisation of the random graph to have $\binom{n}{2}\p$ edges.
When increasing the probability $\p$ from $0$ to $1$ the resulting random graph undergoes the same so-called phase transitions
discovered by Erdös and Renyi.

While the results of the original paper hold for small and large $M(n)$,
the critical phase transistion happening around $M(n) = \frac{n}{2}$ or $\p = \frac{1}{n}$
was later discovered to be imprecise.
\fxnote{Add reference from hofstad somewhere.}
The following section will provide a more precise introduction to the different phases a random graph undergoes
with increasing number of edges.


\section{General results on component sizes}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: General results on component sizes
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section we discuss previous results on component sizes in the Erdös-Renyi random graph.
We begin with the subcritical graph, that is
$\Gcal(n, \p)$ with $\p < \frac{1}{n}$.

Define $\lambda = \p n$ and $I_{\lambda}$ as the large deviation rate functions for Poisson random variables with mean $\lambda$,
that is
\begin{equation}
	I_{\lambda} = \lambda - 1 - \log(\lambda).
\end{equation}

The following theorem provides an upper bound on the size of $\Ccal_{\max}$, the largest component of $\Gcal$.
\begin{theorem}[Lower bound on largest subcritical component, {\cite[Theorem 4.4, p.125]{vanderHofstad.2016}}]
	Fix $\lambda < 1$. 
	Then, for all $a > 1/I_{\lambda}$, 
	there exists $\delta = \delta(a, \lambda)$ such that
	\begin{equation}
		\Prob( |\Ccal_{\max}| \geq a \log n ) = \BigO{n^{-\delta}}.
	\end{equation}
\end{theorem}

The next theorem gives an upper bound for the size of $\Ccal_{\max}$.
\begin{theorem}[Upper bound on largest subcritical component, {\cite[Theorem 4.5, p.125]{vanderHofstad.2016}}]
	Fix $\lambda < 1$. 
	Then, for all $a < 1/I_{\lambda}$, 
	there exists $\delta = \delta(a, \lambda)$ such that
	\begin{equation}
	\Prob( |\Ccal_{\max}| \leq a \log n ) = \BigO{n^{-\delta}}.
	\end{equation}
\end{theorem}

Together, these theorems imply
\begin{equation}
	\frac{|\Ccal_{\max}|}{\log n} \rightarrow_p 1/I_{\lambda}.
\end{equation}

We have established that, for $\p n < 1$, the expected largest component size will be of order $\log n$.

Consider the opposite case, $\lambda = \p n > 1$. 
Denote by $\xi_\lambda = 1 - \eta_n\lambda$ the survival probability of a Poisson branching process with mean offspring $\lambda$.
Note that $\xi_\lambda > 0$ since $\lambda > 1$.
Any vertex is part of a large component with probability $\xi_\lambda$, 
therefore we will expect around $n \xi_\lambda$ vertices being part of large connected components.
The following theorem now states that all of these vertices are, in fact, part of the same connected component, which we call the giant component.
\fxnote{Add explanation for Poisson branching process}
\begin{theorem}[Law of large numbers for giant component, {\cite[Theorem 4.8, p.131]{vanderHofstad.2016}}]
	Fix $\lambda>1$.
	Then, for all $\nu \in (\frac{1}{2}, 1)$, there exists $\delta = \delta(\nu, \lambda)$ such that
	\begin{equation}
		\Prob\left( \left| |\Ccal_{\max}| - n \xi_\lambda \right| \geq n^{\nu}\right) = \BigO{n^{-\delta}}.
	\end{equation}
\end{theorem}

\fxnote{What size is this giant component?}

To recap: For $n \p < 1$ we expect many small clusters of size at most $\log n$,
for $n \p > 1$ we expect one giant component, rapidly approaching size $n$.
But what happens around $n \p \approx 1$?
As it turns out, the emergence of the giant component occurs quite rapidly,
such that shortly after $n \p = 1$ most graphs do not have any component of order between $\frac{1}{2}\n{2}{3}$ and $\n{2}{3}$.

The following theorem provides an approximation of the time of emergence of the giant component,
seeing the random graph on $n$ vertices as a graph process,
starting at $t=0$ with $0$ edges, adding one random edge at every time step.
We would therefore expect the emergence starting around time $\binom{2}{n}\frac{1}{n} \approx \frac{1}{2}n$. 

\begin{theorem}[Emergence of the giant component, {\cite[Theorem 6.8, p.142]{Bollobas.2001}}]
	Almost every graph process $\Gcal = (\Gcal_t)_0^n$ is such that 
	for every $t \geq t_1 = \floor{ n/2 + 2(\log n)^{1/2}\n{2}{3} }$ 
	the graph $\Gcal_t$ has a unique component of order at least $\n{2}{3}$ and the other components have at most $\frac{1}{2}\n{2}{3}$
\end{theorem}

As it turns out, there is a so-called critical window in which the component sizes are not of size $\log n$ and there is no giant component yet.
We call a random graph $\Gcal(n, n^{-1} + t\n{-4}{3})$ critical for $t \in \Real$.
The next theorem provides a approximation of the size of the largest component in a critical random graph.

\begin{theorem}[Largest critical cluster, {\cite[Theorem 5.1, p.150]{vanderHofstad.2016}}] \label{T: largest critical cluster}
	Let $\lambda = 1 + t\n{-1}{3}$, with $t \in \Real$.
	There exists a constant $b = b(t)$ such that for all $\omega > 1$,
	\begin{equation}
		\Prob\left( \omega^{-1} \n{2}{3} \leq |\Ccal_{\max}| \leq \omega \n{2}{3}\right) \geq 1 - \frac{b}{\omega}.
	\end{equation}	
\end{theorem}

Therefore, rescaling the size of the largest component by $\n{-2}{3}$,
it is contained in an interval $[\omega^{-1}, \omega]$ with hight probability.

But is it possible to provide a similar statement not only for the largest, but for all components in a critical random graph?
Aldous notes that previous to his paper
the convergence of the rescaled component sizes to some limit process was generally assumed to be true,
although never explicitly proven.
He therefore provides the following \emph{folk theorem},
which will be proven in more precise form in the course of this thesis.

\begin{folktheorem} \label{T: folk theorem}
	Let $\Cnt(1) \geq \Cnt(2) \geq \dots$ be the ordered component sizes of
	$\Gcal(n, n^{-1} + t\n{-4}{3})$ and let $\sigmant(j)$ be the surplus of the corresponding component.
	Then
	\begin{equation}
		( \n{-2}{3} ( \Cnt(j), \sigmant(j) ), \; j \geq 1 ) 
		\rightarrow_d
		( (\Ct(j), \sigmat), \; j \geq 1 )
		= ( \Ctbold, \sigmatbold ),
	\end{equation}
	as $n \rightarrow \infty$ for some limit $( \Ctbold, \sigmatbold )$
	with $0 < \Ct(j) < \infty$ and $0 \leq \sigmatbold(j) < \infty$ almost surely for each $j \geq 1$.
\end{folktheorem}



\section{Main statements of this thesis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: Main statements of this thesis
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section we state the main results of this thesis,
which is a refinement of Theorem~\ref{T: largest critical cluster}.
While previously the size of largest component was only estimated to be of order $\n{2}{3}$,
the following theorem will give a limit distribution for all component sizes, downscaled by $\n{-2}{3}$.

Denote by $W$ the standard Brownian motion. 
For a fixed parameter $t \in \Real$ we call the process $\Wt$, defined by
\begin{equation}
	\Wt(s) := W(s) + \int_{0}^{s}(t-s)ds = W(s) + ts - \frac{1}{2}s^2,
\end{equation}
the Brownian motion with drift $t-s$ at time $s$.
The central object of our analysis will be the process $\Wt$ and its excursions above past minima.
We reflect $\Wt$ at $0$, defining the process $\Bt$ by
\begin{equation}
	\Bt(s) := \Wt(s) - \min_{u \leq s}\Wt(u)
\end{equation}
and calling it the reflected Brownian motion with drift.

\begin{figure}%
	\centering
	\subfloat[$W(s)$]{\input{figures/fig_bm.tex}}%
	\quad
	\subfloat[$W^{t_1}(s)$]{\input{figures/fig_bm_drift_pos.tex}}%
	\quad
	\subfloat[$B^{t_1}(s)$]{\input{figures/fig_bm_drift_ref_pos.tex}}%
	\quad
	\subfloat[$W^{t_2}(s)$]{\input{figures/fig_bm_drift_neg.tex}}%
	\quad
	\subfloat[$B^{t_2}(s)$]{\input{figures/fig_bm_drift_ref_neg.tex}}%
	\caption{A sample Brownian motion, Brownian motion with drift and with reflection for $t_1>0$ and $t_2<0$.}%
	\label{F: BM}%
\end{figure}

See Figure~\ref{F: BM} for an example of a Brownian motion with drift for positive and negative $t$ and the corresponding reflected process.
Note that for positive $t$ the time intervals between zeroes of $\Bt$ are much longer than for negative $t$.
We call an excursion $\gamma$ of $\Bt$ a time interval $[l(\gamma), r(\gamma)]$ for which $\Bt(l(\gamma)) = \Bt(r(\gamma)) = 0$
and $\Bt(s) > 0$ for all $l(\gamma) < s < r(\gamma)$.
Denote by $|\gamma| = r(\gamma) - l(\gamma)$ the length of an excursion.

Additionally we define a Poisson counting process $\Nt$, 
which equips each excursion with a number of marks,
emerging with intensity $\Bt(s)$ at time $s$.
Informally speaking, the chance of encountering a mark in a time interval $\sds$ is characterized by
\begin{equation}
	\Prob( \text{Some mark emerges in} \; \sds \cond \Bt(u), u \leq s ) = \Bt(s)ds.
\end{equation}
More formally we define $\Nt$ to be the counting process for which
\begin{equation}
	\Nt(s) - \int_{0}^{s} \Bt(s)ds
\end{equation}
is a martingale.
Denote by $\mu(\gamma)$ the number of marks during an excursion $\gamma$.

We now state the main theorem of this thesis, which we will prove gradually in the following chapters.
\begin{theorem}[Main theorem] \label{T: Main}
	Let 
	$\Cnt(1) \geq \Cnt(2) \geq ... $ 
	be the ordered component sizes of 
	$\Gnt$
	and let
	$\sigmant(j)$ 
	be the surplus of the corresponding component.
	Then, as $n \rightarrow \infty$,
	\begin{equation}
	( \n{-2}{3} ( \Cnt(j), \sigmant(j) ), \; j \geq 1 ) 
	\rightarrow_d
	( (\Ct(j), \sigmat), \; j \geq 1 )
	= ( \Ctbold, \sigmatbold ),
	\end{equation}
	where the convergence 
	$\n{-2}{3} \Cntbold \rightarrow_p \Ctbold$
	holds with respect to the $\ld$ topology.
	
	The limit
	$\left( ( \Ct(j), \sigmat(j) ), \; j \geq 1 \right)$
	is distributed as the sequence
	$ \left( (|\gamma_j|, \mu(\gamma_j)), j \geq 1  \right) $
	of lengths and mark-counts of excursions of $\Bt$.
\end{theorem}
\fxfatal{ld topology must be explained or removed.}

We conclude this chapter with an overview of the remaining chapters and the structure of the proof of Theorem~\ref{T: Main}.

Chapter~\ref{C: preliminaries} will develop some preliminary theory on the function spaces $C$ and $D$,
convergence of probability measures and counting processes, which will prove useful in the coming chapters.

In Chapter~\ref{C: bf-walk} we define a way to traverse all vertices of a given graph, called the breadth-first walk $\znt$, 
that reduces the graph to a one-dimensional random walk in which component sizes are decoded as excursions above past minima.
We analyse its characteristics when applied to $\Gnt$ and discover that, after a certain rescaling, it converges in distribution to $\Wt$.

Chapter~\ref{C: surplus edges} deals with the second coordinate $\sigmant$ in Theorem~\ref{T: Main}, the surplus edges.
We describe a Poisson counting process $\nnt$ which tallies up all encountered excess edges and calculate its limit rate as $\Bt$.
The remainder of this chapter will be spent proving that this convergence of rates suffices to declare the convergence of the joint distribution of
the rescaled breadth-first walk and this counting process to $\Wt$ and $\Nt$.

It remains to be proven that not only does the rescaled random walk converge in distribution to the Brownian motion with drift,
but that the convergence of component sizes to lengths of excursions of $\Bt$ in distribution follows as well.
In Chapter~\ref{C: convergence} we first prove that this does indeed hold if we must not expect any large components to "wander off to infinity" as $n \rightarrow \infty$ and subsequently that, with high probability, this problem does not arise in $\Gnt$.
This completes the proof of Theorem~\ref{T: Main}.

Lastly, Chapter~\ref{C: outlook} provides an overview over the remaining statements of Aldous paper,
which contains a non-uniform version of Theorem~\ref{T: Main} and the multiplicative coalescent, 
a process describing the joining of components to form larger components as the parameter $t$ grows.

