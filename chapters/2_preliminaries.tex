% Chapter 2: Preliminaries
% Contains:
%   Weak convergence
%   Vague convergence
%   etc.

\chapter{Preliminaries} \label{C: preliminaries}
\fxnote{Update title.} 

\section{Weak convergence of probability measures}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: On weak convergence
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The central notion of this thesis is the convergence in distribution of random variables in general metric spaces.

\begin{definition}[Weak convergence, {\cite[p.7]{Billingsley.1999}}]
	Let $\mu_n$, $\mu$ be measures on a metric space $S$ with associated \Bosi $\mathcal{S} = \Bo(S)$.
	
	Denote by $\Cb(S)$ the space of bounded, continuous functions $f: S \rightarrow \Real$.
	
	We say $\mu_n$ converges weakly to $\mu$, $\mu_n \Rightarrow \mu$, if
	\begin{equation}
		\int_S fd\mu_n \rightarrow \int_s fd\mu
	\end{equation}
	as $n \rightarrow \infty$ for all $f \in \Cb(S)$.
\end{definition}

We say a random variable $X$ in $(S, \mathcal{S})$, 
defined on a probability space $\ProbSpace$,
has distribution or law $P$ if
\begin{equation}
	P(A) := \Prob(X \in A)
\end{equation}
for all $A \in \mathcal{S}$.

For a series of random variables in the same metric space to converge in distribution 
does not require them to be defined on the same probability space.
Let $X_n$, $X$ be random variables in a metric space $(S, \mathcal{S})$,
\begin{equation}
\begin{aligned}
	X: \ProbSpace &\rightarrow (S, \mathcal{S}), \\
	X_i: (\Omega_i, \SigmaAlgebra_i, \Prob_i) &\rightarrow (S, \mathcal{S}),
\end{aligned}
\end{equation}
with associated distributions $P_n$, $P$.
Now $X_n$ converges in distribution to $X$, $X_n \rightarrow_d X$,
if their distribution measures converge weakly, $P_n \Rightarrow P$.

The main goal of this chapter is now to find necessary and sufficient conditions
for the convergence in distribution of a given sequence of random variables 
or equivalently the weak convergence of their distribution measures.

A first useful reference provides the so-called Portmanteau theorem.
\begin{theorem}[Portmanteau, {\cite[Theorem 2.1, p.16]{Billingsley.1999}}]
	Let $P_n$, $P$ be probability measures on $(S, \mathcal{S})$.
	The following conditions are equivalent:
	\begin{enumerate}
		\item $P_n \Rightarrow P$,
		\item $\int_S fdP_n \rightarrow \int_S fdP$ for all $f \in \Cb(S)$,
		\item $\limsup_n P_n(F) \leq P(F)$ for all closed $F$,
		\item $\liminf_n P_n(G) \geq P(G)$ for all open $G$,
		\item $P_n(A) \rightarrow P(A)$ for all $A$ with $P(\partial A) = 0$.
	\end{enumerate}
\end{theorem}

A powerful tool to prove weak convergence of measures is relative compactness.

\begin{definition}[Relative compactness, {\cite[p.57]{Billingsley.1999}}] \label{D: Rel Compactness}
	Let $\Pi$ be a family of probability measures on $(S, \mathcal{S})$.
	We call $\Pi$ \emph{relatively compact} if for every sequence in $\Pi$ there exists a convergent subsequence.
	That is, for all sequences $\{P_n\} \subset \Pi$ there exists $\{P_{n_i} \} \subset \Pi$
	and a probability measure $Q$ on $(S, \mathcal{S})$, not necessarily on $\Pi$,
	such that $P_{n_i} \Rightarrow_i Q$.
	
	We call a sequence of probability measures $\{P_n\}$ relatively compact if
	for every subsequence $\{P_{n_i}\}$ there exists a further subsequence
	$\{P_{n_{i_k}}\}$ and a probabilty measure $Q$ such that
	$P_{n_{i_k}} \Rightarrow_k Q$.
\end{definition}

For function spaces, knowing that a sequence of functions is relatively compact
provides us with a powerful tool to prove convergence in distribution.

\begin{definition}[Finite dimensional distributions, {\cite{Aldous.1997}}]
	\fxfatal{Add real reference here.}
	Let $X: \Omega \times \Rplus \rightarrow \mathbb{X}$ be a stochastic process with distribution $P$
	For $t_1, \dots, t_k \in \Rplus$ we denote by \emph{finite-dimensional distributions} of $X$ 
	the push forward measures 
	\begin{equation}
		P\pi^{-1}_{t_1, \dots, t_k}(A) := \Prob\{ X(t_1) \in A_1, \dots, X(t_k) \in A_k \}
	\end{equation}
	for $A = A_1 \times \dots \times A_k \in \mathbb{X}^k$.
\end{definition}

Now consider a relatively compact sequence of stochastic processes $\{X_n\}$,
with distributions $\{P_n\}$,
and a stochastic process $X$ with distribution $P$,
such that
\begin{equation}
	P_n\pi^{-1}_{t_1, \dots, t_k} \Rightarrow P\pi^{-1}_{t_1, \dots, t_k}
\end{equation}
for all $k$ and $t_1, \dots, t_k \in \Rplus$.
Since $\{P_n\}$ is relatively compact,
we know that every subsequence contains a further subsequence converging to some probability measure $Q$.
It can be shown that the convergence of finite-dimensional distributions implies that all of these
limit measure are in fact $P$,
which in turn proves $P_n \Rightarrow P$. For details see \cite[p.57]{Billingsley.1999}.

If we can prove a series of probability measure to converge in finite-dimensional distributions to some
limit measure we therefore only need to show relative compactness of the series
for the convergence in distribution to hold.

Relative compactness is closely linked to another property of series of measures, tightness.

We call a measure $\mu$ on a metric space tight,
if for all $\epsilon > 0$ there exists a compact $K$ such that
\begin{equation}
	\mu(K^c) < \epsilon.
\end{equation}

This carries over to families of probability measures as follows:

\begin{definition}[Tightness of families of probability measures, {\cite[p.59]{Billingsley.1999}}] \label{D: Tightness}
	A family $\Pi$ of probability measures on a metric space $(S, \mathcal{S})$ is \emph{tight} 
	if for every $\epsilon > 0$ there exists a compact $K \subset S$
	such that $P(K) > 1 - \epsilon$
	for all $P \in \Pi$.	
\end{definition}

The last main result of this section now provides a link between tightness and relative compactness
and therefore an effective means of proving convergence in distribution.

\begin{theorem}[Prohorov's theorem, {\cite[Theorem 5.1, p.59]{Billingsley.1999}}] \label{T: Prohorov}
	Let $P_n$ be a series of probability measures on a metric space $(S, \mathcal{S})$. 
	If $P_n$ is tight, it is relatively compact.
\end{theorem}

We summarize this theorem and our considerations above in the following lemma.

\begin{lemma}
	Let $X, X_n$ be stochastic processes with distributions $P, P_n$.
	If $\{X_n\}$  is tight and
	\begin{equation*}
		P_n\pi^{-1}_{t_1, \dots, t_k} \Rightarrow P\pi^{-1}_{t_1, \dots, t_k},
	\end{equation*}
	for all $k$ and $t_1, \dots, t_k \in \Rplus$,
	then $X_n \Rightarrow X$.
\end{lemma}


\section{The space $\DT$}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: The space D
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{definition}[The space $\DT$]
	
\end{definition}

Compact sets of $\DT$.

\begin{definition}[Skorohod metric]
	
\end{definition}

\begin{lemma}[Tightness in $\DT$]
	
\end{lemma}



\section{Point processes}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: Point processes
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{definition}[Point process, {\cite[p.123]{Resnick.2008}}]
	Let $S$ be a locally compact second countable Hausdorff space
	and $\Bo(S)$ its \Bosi.
	Let $\{x_i, i \geq 1\}$ be a collection of points of $S$.
	Let
	\begin{equation}
		\mu := \sum_{i \geq 1} \delta_{x_i},
	\end{equation}
	with $\delta_x$ the Dirac measure of $x \in S$,
	be locally compact, that is, 
	if $C \in \Bo(S)$ is compact then $\mu(C) < \infty$.
	Then $\mu$ is a \emph{point measure} on $E$.
	
	Denote by $M_p(S)$ the space of all point measures on $E$
	and let $\mathscr{M}_p(S)$ be the smallest $\sigma$-algebra containing all sets of the form
	\begin{equation*}
		\{ m \in M_p(S) \cond m(A) \in B \}
	\end{equation*}
	for some $A \in \Bo(S)$ and $B \in \Bo(\Rplus)$.
	
	A \emph{point process} $N$ is a measurable map from a probability space
	$\ProbSpace$ into $(M_p(S), \mathscr{M}_p(S))$.
\end{definition}

For all coming observations, we will take $\Rplus$ as th underlying Hausdorff space with its \Bosi $\Bo = \Bo(\Rplus)$.

\begin{definition}[Poisson point process, {\cite[p.130]{Resnick.2008}}]
	We call a point process $N$ a \emph{Poisson point process} or \emph{Poisson process},
	if
	\begin{enumerate}
		\item for all disjoint sets $A_1, A_2, \dots, A_n \in \Bo$
			the random variables $N(A_1), N(A_2), \dots, N(A_n)$ are independent and
		\item for all $A \in \Bo$, $N(A)$ has Poisson distribution $\Poisson(\gamma)$,
			\begin{equation*}
				\Prob(N(A) = k) = \frac{\gamma^k}{k!}\exp(-\gamma)
			\end{equation*}
			where $\gamma = \gamma(A) \in [0, \infty]$ is the \emph{mean measure} or \emph{intensity} of $N$.
	\end{enumerate}
	The mean measure is often given in terms of a \emph{rate} or \emph{conditional intenstiy} $\lambda$ by
	\begin{equation}
		\gamma(A) = \int_A \lambda(t)dt.
	\end{equation}	
\end{definition}

\begin{definition}[Simple point process, {\cite[p.124]{Resnick.2008}}]
	A point process $N$ on $\Rplus$ is called \emph{simple} if 
	\begin{equation}
		\Prob( N({x}) > 1 ) = 0
	\end{equation}
	for all $x \in \Rplus$.
\end{definition}

By \cite[Remark 2.1, p.34]{Haenggi.2013} a Poisson point process is simple if and only if
its mean measure $\gamma$ has no discrete component,
that is $\gamma(x) = 0$ for all $x \in \Rplus$.


\begin{definition}[Vague convergence]
	Let $\CK(\Rplus)$ be the space of continuous real valued functions on $\Rplus$ with compact support,
	meaning there exists a compact set $K \in \Bo$ such that $f(x) = 0$ for all $x \notin K$.
	
	Let $\mu, \mu_1, \mu_2, \dots$ be point measures on a Hausdorff space $E$.
	We say $\mu_n$ converges vaguely to $\mu$, $\mu_n \rightarrow_v \mu$, if
	\begin{equation}
		\int_{\Rplus} fd\mu_n \xrightarrow{n \rightarrow \infty} \int_{\Rplus} fd\mu
	\end{equation}
	for all $f \in \CK(\Rplus)$.
\end{definition}


\begin{lemma}[Equivalent conditions for vague convergence, {\cite[Proposition 3.12, p.142]{Resnick.2008}}]
	Let $\mu, \mu_1, \mu_2, \dots$ be point measures on $E$.
	The following are equivalent:
	\begin{enumerate}
		\item $\mu_n \rightarrow_v \mu$,
		\item $\mu_n(B) \rightarrow \mu(B)$ for all relatively compact (i.e. with compact closure) $B$ 
			for which $\mu(\partial(B)) = 0$.
		\item
	\end{enumerate}
\end{lemma}

\begin{lemma}[Pointwise convergence, {\cite[Proposition 3.13, p.144]{Resnick.2008}}]
	Let $\mu, \mu_1, \mu_2, \dots$ be point measures on $E$ and $\mu_n \rightarrow_v \mu$.
	For compact $K$ with $\mu(\partial K) = 0$ and $n \geq N(K)$
	there exist a labeling of points of $\mu_n$ and $\mu$ in $K$ such that
	\begin{equation}
	\begin{aligned}
		\mu_n(\cdot \cap K) &= \sum_{i=1}^{M} \delta_{x_i^{(n)}}, \\
		\mu(\cdot \cap K) &= \sum_{i=1}^{M} \delta_{x_i},
	\end{aligned}
	\end{equation}
	and in $E^M$
	\begin{equation}
		( x_i^{(n)}, 1 \leq i \leq M ) \xrightarrow{n \rightarrow \infty}
		( x_i, 1 \leq i \leq M  ) 
	\end{equation}
	in the sense of componentwise convergence.
\end{lemma}


\section{Further results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: Further results
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We state the theorem here, without proof, as it appears in \cite[Theorem 1.4, p.339 f.]{Ethier.2005},
omitting one of two equivalent conditions and all references to higher dimensional processes,
in order to focus on the one-dimensional case we will need for our proof.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Central limit theorem for martingales: Statement
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{theorem}[Central limit theorem for martingales] \label{T: functional CLT martingales}
	Let $\{\Fn{t}\}$ be a filtration and $M_n$ a $\{\Fn{t}\}$-local martingale with sample paths in $D_{\Real}[0,\infty)$ and $\Mn{0}=0$.
	Let $B_n$ be a process with sample paths in $D_{\Real}[0,\infty)$, increasing in $t$, such that $M_n^2 - B_n$ is an $\{\Fn{t}\}$-local martingale.
	
	Let the following conditions hold:
	For each $T>0$,
	\begin{equation} \label{E: cond1 CLT}
	\lim_{n->\infty} \ExpBig{
		\sup_{t \leq T} | \Bn{t} - \Bn{t-}|
	} = 0,
	\end{equation}
	\begin{equation} \label{E: cond2 CLT}
	\lim_{n->\infty} \ExpBig{
		\sup_{t \leq T} | \Mn{t} - \Mn{t-}|^2
	} = 0,
	\end{equation}
	and with $c(t)$ a continuous, increasing function on $[0, \infty)$, $c(0) = 0$, let
	\begin{equation} \label{E: cond3 CLT}
	\Bn{t} \longrightarrow_p c(t).
	\end{equation}
	Then $M_n \longrightarrow_d X$ where $X$ is a process with sample paths in $C_{\Real}[0,\infty)$ and independent Gaussian increments.
\end{theorem}

We conclude this chapter by introducing Girsanovs Theorem,
which will enable us to prove certain properties of the Brownian motion with drift.
We state the theorem as it appears in \cite[Theorem 4.2.2, p.66]{Lamberton.2000}.

\begin{theorem}[Girsanov] \label{T: Girsanov}
	Let $(W(s))_{0 \leq s \leq T}$ be a Brownian motion on a probability space $(\Omega, \mathcal{F}, \mathbb{P})$.
	Let $(\Theta(s))_{0 \leq s \leq T}$ be an adapted process satisfying
	\begin{equation} \label{E: Girsanov cond Theta}
	\int_{0}^{T} \Theta^2(u)du < \infty.
	\end{equation}
	Define
	\begin{align}
	\tilde{W}(s) &:= W(s) + \int_0^s \Theta(u)du, \label{E: Girsanov def W tilde} \\ 
	X(s) &:= \exp \left\{ -\int_{0}^{s} \Theta(u) dW(u) - \frac{1}{2} \int_0^s \Theta^2(u)du \right\}. \label{E: Girsanov def X}
	\end{align}
	If $X(s)$ a martingale, that is $\Exp{X(s)} = \Exp{X(0)} = 1$ for all $s$,
	the measure $\mathbb{Q}$, defined by
	\begin{equation} \label{E: Girsanov def P tilde}
	\tilde{\mathbb{P}}(A) := \int_A X(\omega) d\mathbb{P}(\omega), \; \text{for all} \; A \in \mathcal{F},
	\end{equation}
	is a probability measure under which the process 
	$(\tilde{W}(s))_{0 \leq s \leq T}$
	is a Brownian motion.
\end{theorem}

By \cite[Remark 4.2.3, p.66]{Lamberton.2000}, a sufficient condition for $X(t)$ to be a martingale is the so-called Novikov-condition
\begin{equation} \label{E: Novikov}
\ExpBig{ \exp \left( \frac{1}{2} \int_{0}^{T} \Theta^2(u)du \right) } < \infty.
\end{equation}

We can apply this Theorem to the Brownian motion with drift $W^t$ as follows:
Recall the definition
\begin{equation}
\Wt(s) = W(s) + ts - \frac{1}{2} s^2 = W(s) + \int_0^s (t-u)du.
\end{equation}
For $T<\infty$, $\Theta(u) := t-u$ satisfies \eqref{E: Novikov},
therefore $X(t)$, as defined in \eqref{E: Girsanov def X}, is a martingale and
$\Wt$ is a standard Brownian motion under the probability measure $\tilde{\mathbb{P}}$ defined in \eqref{E: Girsanov def P tilde}.
Since $X(s) > 0$ almost surely for all $s$, the probability measures $\mathbb{P}$ and $\tilde{\mathbb{P}}$ agree which sets have probability zero:
\begin{equation} \label{E: P0 = Q0}
\mathbb{P}(A) = 0 \iff \tilde{\mathbb{P}}(A) = 0, \; \text{for all} \; A \in \mathcal{F}.
\end{equation}
Properties holding with probability $1$ or $0$ for a standard Brownian motion will hold with probability $1$ or $0$, respectively,
for $\Wt$ under $\tilde{\mathbb{P}}$ and hence under the original measure $\mathbb{P}$.
