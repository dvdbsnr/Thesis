% Chapter 3: Surplus edges
% Contains:
%   Definition of surplus edges
%   Eligible edges for s.e.
%   Calculating the probability of s.e.
%   The proof of (Z_n, N_n) => (W,N)
%   Why the overestimated probability is OK

\chapter{Surplus edges} \label{C: surplus edges}
\fxnote{Update title.}

The goal of this chapter will be to first examine under which circumstances surplus edges can arise during the breadth-first walk,
then finding an expression for the probability of encountering one 
and finally proving the joint convergence of $\rznt$ and the surplus edge counting process to $\Wt$ and some limit process dependent on the realisation of $\Wt$.


\section{Counting surplus edges}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: Counting surplus edges
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We begin by describing a way to analyse the appearance of surplus edges.
In Chapter~\ref{C: bf-walk} we defined the breadth-first walk $Z_n$, 
which counted new connections to previously not connected vertices.
We remind ourselves that a surplus edge in a graph $\Gnt$ appears if,
during the transition of vertices and components by the breadth-first walk,
a vertex forms a new connection to another vertex
which already has opened connections to some explored node.
We associate with $\Gcal$ a counting process $(\nnt(s), 0 \leq s \leq n)$,
with $\nnt(0) = 0$, which increases by $1$ at each appearance of a surplus edge.

\begin{figure}[ht]
	\input{figures/chapter4/fig_tree.tex}
	\caption{A sample component} 
	\label{F: Surplus Edges Tree}
\end{figure}

To understand the number of vertices which can even open such excess connections, 
consider the breadth-first walk on the graph of Figure~\ref{F: Surplus Edges Tree} at time $s=2$.
The children of $v_1$, $v_2$ to $v_4$, and the children of $v_2$, that is $v_5$ and $v_6$, are already discovered.
We are interested in surplus edges to $v_3$.
Since $v_1$ to $v_6$ are unable to form edges to become children of $v_3$, we have $\Ineligible{2} = |\{ v_1, \dots,  v_6\}|$.
Of these vertices, $v_1$ and $v_2$ are already explored and every connection to neighbouring nodes is known.
Vertex $v_3$ can not have an edge to itself, so only $v_4$, $v_5$ and $v_6$ are eligible to receive a surplus edge to $v_3$.

Let us examine what these considerations mean in terms of the breadth-first walk $\znt$.
When starting at a new component there are no vertices eligible for a surplus edge.
For each new vertex found as a member of this component we have one additional eligible node 
and with each step taken, one more vertex is explored and thus can no longer receive a surplus edge.
The number of vertices eligible for an excess edge therefore corresponds to the level of the breadth-first walk above its past minimum,
which is attained at the beginning of the component.
Hence we will expect the probability of encountering an excess edge at time $s$ to be proportionate to
\begin{equation}
\zminz(s) := \znt(s) - \min_{u \leq s}\znt(u).
\end{equation}
Rescaling the counting process appropriately this probability should scale to
\begin{equation}
\rzminz(s) := \rznt(s) - \min_{u \leq s}\rznt(u),
\end{equation}
which converges in distribution to
\begin{equation}
\Bt(s) := \Wt(s) - \min_{u \leq s}\Wt(u). 
\end{equation}

In chapter~\ref{C: bf-walk} we examined a similar process, $B_n$, 
which increased by one for each appearance of a new edge to a previously not connected vertex.
Lemma~\ref{L: formula an} established that $\Bn(s) = \int_{0}^{s} \an{u}du$ with
\begin{equation*}
	\an(s) = (n - \Ineligible{s}) \ps,
\end{equation*}
where $\Ineligible{s}$ is the number of vertices ineligible to become a child of $v(\ceil{s})$ at time $s$.
In terms of counting processes, we call $\an$ the rate or conditional intensity of $\Bn$.
It is evident that $\nnt$ will have a similar rate,
substituting the number of vertices eligible to become a child of $v(\ceil{s})$ with the number of vertices eligible to receive a surplus edge to $v(\ceil{s})$.

In general, at time $i-1$, the first $i$ vertices are ineligible for a surplus edge to $v(i)$.
The remaining $\Ineligible{i-1} - i$ vertices are candidates for an excess edge opening with probability $\p$.
Therefore, the counting process $\nnt$ has rate
\begin{equation} \label{E: rate Nnt}
\lambda(s) = (\Ineligible{\floor{s}} - \floor{s}) \ps.
\end{equation}
Note that this rate is only exact for the chance of encountering exactly one surplus edge, 
but an overestimation for multiple excess edges.
If we encounter a surplus edge at some time $s \in [i-1, i)$,
the number of eligible vertices should decrease by one.
However, \eqref{E: rate Nnt} is constant for all $s \in [i-1, i)$.
For ease of computation we will continue with this overestimation and later argue that the difference becomes negligible as $n \rightarrow \infty$.

Lemma~\ref{L: formula an} established $\Ineligible{s} = s + \Zetan{\ceil{s}} + \Zn{s}$ 
and using \eqref{E: zeta-1(i) = 1 - min(j)} we can rewrite
\begin{equation*}
\begin{aligned}
\Ineligible{\floor{s}} - \floor{s} 
&= \floor{s} - \ZetaMinus{\floor{s}+1} + \znt(\floor{s}) - \floor{s} \\
&= 1 - \min_{u \leq \floor{s}} \znt(u) + \znt(\floor{s}),
\end{aligned}
\end{equation*}
and the conditional intensity becomes
\begin{equation} \label{E: rate Nnt 2}
\lambda(s) = (1 - \min_{u \leq \floor{s}} \znt(u) + \znt(\floor{s}) \ps.
\end{equation}
We now rescale the counting process via
\begin{equation} \label{E: rescale Nnt}
\rnnt(s) = \nnt(\n{2}{3}s).
\end{equation}
We calculate the rate of this rescaled process.
Recall that the conditional intensity $\bar{\lambda}(s)$ of the process $\rnnt(s)$ must satisfy
\begin{equation}
\Exp{\rnnt(s)} = \int_{0}^{s} \bar{\lambda}(u) du.
\end{equation}
Using \eqref{E: rescale Nnt} we evaluate the above integral in terms of $\lambda(s)$:
\begin{equation*}
\begin{aligned}
\Exp{\rnnt(s)}
&= \Exp{\nnt(\n{2}{3}s)} \\
&= \int_{0}^{\n{2}{3}s} \lambda(u) du \\
&= \int_{0}^{s} \n{2}{3} \lambda(\n{2}{3}u) du.
\end{aligned}
\end{equation*}
Comparing both integrands gives us
\begin{equation} \label{E: conditional intensity final}
\begin{aligned}
\bar{\lambda}(s)
&= \n{2}{3} \lambda(\n{2}{3}s) \\
&= \n{2}{3} \frac{1 - \min_{u \leq \n{2}{3}s} \znt(u) + \znt(\n{2}{3}s)}{1 - (\n{2}{3}s - \floor{\n{2}{3}s})\p} \p \\
&= \n{2}{3} \frac{1 - \n{1}{3} \min_{u \leq s} \rznt(u) + \n{1}{3}\rznt(s)}{1 - (\n{2}{3}s - \floor{\n{2}{3}s})\p} \p \\
&= n \p \frac{\n{-1}{3} - \min_{u \leq s} \rznt(u) + \rznt(s)}{1 - (\n{2}{3}s - \floor{\n{2}{3}s})\p}
\end{aligned}
\end{equation}

Since $n\p \rightarrow 1$ and $|\n{2}{3}s - \floor{\n{2}{3}s}| < 1$ for all $s$ and $n$,
this rate becomes asymptotically close to $\rznt(s) - \min_{u \leq s}\rznt(u)$ as $n \rightarrow \infty$.

By Theorem~\ref{T: Z -> W} we have $\rznt \rightarrow_d \Wt$,
so
\begin{equation}
\bar{\lambda}(s) \rightarrow_d \Wt(s) - \min_{u \leq s}\Wt(u) = \Bt(s).
\end{equation}
The rate of the counting process $\rnnt$ therefore converges in distribution to $\Bt$.




\section{Weak convergence of $(Z^t_n, N^t_n)$}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: Weak convergence of $(Z^t_n, N^t_n)$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Theorem Joint Convergence: Statement
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{theorem} \label{T: Joint Convergence}
	For the previously defined processes $\rznt$ and $\rnnt$,
	the joint weak convergence
	\begin{equation}
	( \rznt(s), \rnnt(s); s \geq 0 ) \longrightarrow_d (\Wt(s), \Nt(s); s \geq 0)
	\end{equation}
	holds, where $\Nt$ is the counting process with conditional intensity $\Bt$,
	i.e. the process for which
	\begin{equation*}
	\Nt(s) - \int_{0}^s \Bt(u)du
	\end{equation*}
	is a martingale.
\end{theorem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Theorem Joint Convergence: Proof
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% OVERVIEW
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We begin with a quick overview of the proof.

We want to show that
\begin{equation}
(\rznt, \rnnt) \rightarrow_d (\Wt, \Nt),
\end{equation}
meaning 
\begin{equation}
\Exp{f(\rznt, \rnnt)} \xrightarrow{n \rightarrow \infty} \Exp{f(\Wt, \Nt)}
\end{equation}
for all continuous, bounded functions $f:D[0,T]^2\rightarrow\Real$.
The main idea of our proof is conditioning the expectations of some fixed realization of the Brownian motion $\Wt$ and analysing  $\Exp{f(\znt, \nnt) \cond \Wt}$.
We first prove that for all $\epsilon > 0$ and sufficiently large $n$,
\begin{equation}
	| \Exp{f(\znt, \nnt) \cond \Wt} -  \Exp{f(\Wt, \nnt) \cond \Wt} | < \epsilon
\end{equation}
holds.
We then show that, conditioning on a fixed underlying Brownian motion $\Wt$, the convergence $\nnt \rightarrow_d \Nt$ holds,
which proves
\begin{equation}
	\Exp{f(\Wt, \nnt) \cond \Wt} \xrightarrow{n \rightarrow \infty} \Exp{f(\Wt, \Nt ) \cond \Wt}
\end{equation}
since $f(\Wt, \cdot)$ is a bounded and continuous function.
The Theorem follows by averaging over all $\Wt$ using the law of total expectation.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proofpart 1: Tightness of N
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proofpart}
First, we will show that $\rnnt$ is tight as a random process with image in $\DT$.
We already know that $\rznt \rightarrow_d \Wt$, which implies that $\rznt$ is tight, 
so for all $\epsilon>0$ there exists a compact $K \subset \DT$ such that
\begin{equation} \label{E: rznt tight}
\inf_n \Prob ( \rznt \in K) > 1 - \epsilon.
\end{equation}

To show that $\rnnt$ is tight, we need to prove that for all $\epsilon>0$ there exists a $K \subset \DT$ such that
\begin{equation}
	\Prob(\rnnt \in K) > 1 - \epsilon.
\end{equation}
By the Arzelá–Ascoli theorem, it suffices to show that there exists a real number $K>0$, such that
\begin{equation}
	\Prob( \sup_{s \leq T} \rnnt(s) < K ) > 1 - \epsilon. 
\end{equation}
Since $\rnnt(s)$ is an increasing process in $s$, this is equivalent to
\begin{equation}
	\Prob( \rnnt(T) \geq K ) < \epsilon.
\end{equation}

We establish an analogous result for $\rznt$ from \eqref{E: rznt tight}.
For all $\epsilon > 0$ exists $A>0$ such that for all $n$
\begin{equation} \label{E: rznt bounded}
\Prob(\sup_{s \leq T} |\rznt(s)| > A) < \epsilon.
\end{equation}

Define $\rzminz(s) := \rznt(s) - \min_{u \leq s}\rznt(u)$, the process reflecting $\rznt$ at the x-axis.
Since $|\rzminz(s)| \leq 2\max_{u \leq s}|\rznt(u)|$, \eqref{E: rznt bounded} implies
\begin{equation} \label{E: rzminz bounded}
\Prob(\sup_{s \leq T} |\rzminz(s)| > 2A) < \epsilon.
\end{equation}
Therefore, for all $\epsilon > 0$ exists an $A>0$ such that
\begin{equation}
\Prob( \sup_{s \leq T} |\zminz(\n{2}{3}s)| \geq A\n{1}{3} ) < \epsilon
\end{equation}
holds for all $n \in \Nat$.

We now move to the process $\rnnt$. Consider the unscaled process $\nnt$ at time $i \in [0, \n{2}{3}T]$.
The increment to its next step is binomially distributed on the number of vertices eligible for a surplus edge:
\begin{equation}
\nnt(i) - \nnt(i-1) \sim \Binom(\zminz(i-1), \p).
\end{equation}
As previously established, for all $i \in [0, \n{2}{3}T]$ we know 
\begin{equation}
	\zminz(i) \leq \sup_{j \leq \n{2}{3}T} \zminz(j) \leq A\n{1}{3}
\end{equation}
with probability greater than $1-\epsilon$.

If we condition on the event that $\zminz(i-1) \leq A\n{1}{3}$, a random variable $X_i \sim \Binom(\zminz(i-1), \p)$ will be stochastically dominated:
\begin{equation}
X_i \leq_{\text{st.}} Y_i \sim \Binom(A\n{1}{3}, \p).
\end{equation}
Seeing $\nnt(T\n{2}{3})$ as the sum of all its increments, we arrive at
\begin{equation} \label{E: nnt stoch dominance}
\nnt(T\n{2}{3}) \leq_{\text{st.}} \sum_{j=1}^{T\n{2}{3}} Y_j,
\end{equation}
where $Y_1, Y_2, \dots, Y_{T\n{2}{3}} \sim \Binom(A\n{1}{3}, \p)$.

We denote by $\Event{A}$ the event $\sup_{j \leq \n{2}{3}T} \zminz(j) \leq A\n{1}{3}$ and use the law of total probability to compute
\begin{equation} \label{E: First Total Probability Argument}
\begin{aligned}
\Prob(\rnnt(T) \geq K) 
&= \Prob(\nnt(\n{2}{3}T) \geq K\n{1}{3}) \\
&=  \Prob(\nnt(\n{2}{3}T) \geq K\n{1}{3} \cond \Event{A}) \Prob(\Event{A}) \\
&\quad + \Prob(\nnt(\n{2}{3}T) \geq K\n{1}{3} \cond \neg\Event{A}) \Prob(\neg \Event{A}) \\
&\leq \Prob(\nnt(\n{2}{3}T) \geq K\n{1}{3} \cond \Event{A}) + \epsilon, \\
\end{aligned}	
\end{equation}

which holds since $\Prob(\neg\Event{A}) <\epsilon$.

Since this probability is now conditioned on $\Event{A}$, the stochastic dominance \eqref{E: nnt stoch dominance} holds.
Markov's inequality then gives
\begin{align*}
\Prob(\nnt(\n{2}{3}T) \geq K\n{1}{3} \cond \Event{A}) 
&\leq \Prob( \sum_{j=1}^{T\n{2}{3}} Y_j \geq K) \\
&\leq \frac{1}{K}T\n{2}{3} \Exp{Y_1} \\
&= \frac{1}{K}T\n{2}{3} \p A\n{1}{3} \\
&= \frac{1}{K}n\p TA \\
&\leq \frac{1}{K}CTA
\end{align*}
for some constant $C \in \Real$, since $n\p \rightarrow 1$ as $n \rightarrow \infty$.
\fxnote{Fix the use of > and >= here.}
\fxnote{Fix the use of A/K here.}
So
\begin{equation}
\Prob(\rnnt(T) \geq K) \leq \epsilon + \frac{1}{K}CTA \leq 2 \epsilon
\end{equation}
for large $K$.
With high probability, $\sup_{s \leq T}\rnnt(s)$ is bounded by some $K>0$,
hence the random variable $\rnnt$ maps into a compact subset of $\DT$ with high probability and $\rnnt$ is tight.

Since both $\rznt$ and $\rnnt$ are tight, $(\rznt, \rnnt)$ is tight.
Thus there is a compact subset $C \subset \DT^2$ such that
\begin{equation}
	\Prob( (\rznt, \rnnt) \in C) > 1 - \epsilon.
\end{equation}
\end{proofpart}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proofpart 2: Estimation of expectation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proofpart}
The next step in our proof will be showing that for all $\epsilon > 0$,
\begin{equation}
|\Exp{ f(\rznt, \rnnt) \cond \Wt} - \Exp{f(\Wt, \rnnt) \cond \Wt}| < \epsilon,
\end{equation}
for sufficiently large $n$.
Recall that $f$ is a bounded function, so there exists $M>0$ such that $|f(x,y)| \leq M$ for all $(x,y) \in \DT^2$.
We denote by $\Event{C}$ the event $(\znt, \rnnt) \in C$ and use the law of total expectation to calculate
\begin{equation} \label{E: Estimation step 1}
\begin{aligned}
\Exp{ f(\rznt, \rnnt) \cond \Wt} 
&= \Exp{f(\rznt, \rnnt) \cond \Wt, \Event{C}} \Prob(\Event{C}) \\
&\quad + \Exp{f(\rznt, \rnnt) \cond \Wt, \neg\Event{C}} \Prob(\neg\Event{C}) \\
&\leq \Exp{f(\rznt, \rnnt) \cond \Wt, \Event{C}}\Prob(\Event{C}) + \epsilon M.
\end{aligned}
\end{equation}

Since $\rznt \rightarrow_d \Wt$, we can use the Skorohod representation theorem to define random variables 
$\WtX$, $\bar{\mathcal{Z}}^t_1$, $\bar{\mathcal{Z}}^t_2$, \dots
on the some common probability space, such that $\WtX \sim \Wt$, $\bar{\mathcal{Z}}^t_i \sim \bar{Z}^t_i$ for all $i \in \Nat$
and $\rzntX \rightarrow_{a.s.} \WtX$ as $n \rightarrow \infty$.
Meaning, since $\WtX$ and $\rzntX$ are random variables mapping into function spaces, we have
\begin{equation}
\sup_{s \leq T} |\rzntX(s) - \WtX(s)| \rightarrow_{a.s.} 0,
\end{equation}
which additionally implies
\begin{equation} 
\sup_{s \leq T} |\rzntX(s) - \WtX(s)| \rightarrow_p 0,
\end{equation}
so for all $\epsilon > 0$:
\begin{equation} \label{E: Prob(Event(delta)) 1}
\Prob( \sup_{s \leq T} |\rzntX(s) - \WtX(s)| > \epsilon ) \rightarrow 0.
\end{equation}

Additionally, we define a process $\rnntX$ on the same probability space as $\WtX$ and $\rzntX$ as a counting process with rate
\begin{equation}
\bar{\lambda}'(s) = n \p \frac{\n{-1}{3} - \min_{u \leq s} \rzntX(u) + \rzntX(s)}{1 - (\n{2}{3}s - \floor{\n{2}{3}s})\p},
\end{equation}
which makes it the equivalent of $\rnnt$ in this new probability space, the rate of which is defined in \eqref{E: conditional intensity final}.
Since $\rzntX \sim \rznt$ we have $\rnntX \sim \rnnt$.

Form here on out we substitute $\Wt$, $\rznt$ and $\rnnt$ with $\WtX$, $\rzntX$ and $\rnntX$ respectively
and denote by $\Event{C}$ the event $(\rzntX, \rnntX) \in C$ for a compact $C \subset \DT^2$.
By the equality in distribution, the final result on the expectation of these processes will then still hold for the original processes.

Defining the norm $\norm{X}_T := \sup_{s \leq T}X(s)$, we denote by $\Event{\delta}$ the event $\norm{\WtX - \rzntX}_T < \delta$.
By \eqref{E: Prob(Event(delta)) 1}, for sufficiently large $n$, 
\begin{equation} \label{E: Prob Event delta}
\Prob(\Event{\delta}) > 1 - \epsilon.
\end{equation}
Now an argument analogous to \eqref{E: Estimation step 1} gives
\begin{equation} \label{E: Estimation step 2}
\begin{aligned}
\Exp{f(\rzntX, \rnntX) \cond \WtX} 
&\leq \Exp{f(\rzntX, \rnntX) \cond \WtX, \Event{C}}\Prob(\Event{C}) + \epsilon M \\
&\leq \Exp{f(\rzntX, \rnntX) \cond \WtX, \Event{C}, \Event{\delta}}\Prob(\Event{C})\Prob(\Event{\delta}) + 2 \epsilon M.
\end{aligned}
\end{equation}

By the continuity of $f$, for all $\epsilon>0$ we can choose a $\delta > 0$ such that $\norm{\WtX - \rzntX}_T < \delta$ implies
that the inequality
\begin{equation*}
\begin{aligned}
f(\rzntX, \rnntX) &= f(\rzntX, \rnntX) + f(\WtX, \rnntX) - f(\WtX, \rnntX) \\
&\leq f(\WtX, \rnntX) + |f(\znt, \rnntX) - f(\WtX, \rnntX)| \\
&\leq f(\WtX, \rnntX) + \epsilon
\end{aligned}
\end{equation*}
holds. Therefore
\begin{equation} \label{E: Estimation step 3}
\begin{aligned}
\Exp{f(\znt, \rnntX) \cond \WtX} 
&\leq \Exp{f(\rzntX, \rnntX) \cond \WtX, \Event{C}, \Event{\delta}}\Prob(\Event{C})\Prob(\Event{\delta}) + 2 \epsilon M \\
&\leq \Exp{f(\WtX, \rnntX) \cond \WtX, \Event{C}, \Event{\delta}}\Prob(\Event{C})\Prob(\Event{\delta}) + \epsilon + 2 \epsilon M.
\end{aligned}
\end{equation}
\end{proofpart}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proofpart 3: Nn ->_d N
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{proofpart}
Our next objective is to establish the convergence $\rnntX \rightarrow_d \Nt$.
For this, we define a process $\Ndis$, which may be thought of as $\Nt$ in discrete time.
While $\Nt$ is the continuous counting process with $\Bt$ as conditional intensity,
we define $\Ndis$ by
\begin{equation}
\begin{aligned}
\Ndis(0) &:= 0, \\
\Ndis(k) &:= \Ndis(k-1) + \xi_k, 
\end{aligned}
\end{equation}
where $\xi_k \sim \Binom(\n{1}{3}\Bt(\n{-2}{3}k), \p)$,
the discrete steps of $\Ndis$ are dependent on an upscaling of the reflected Brownian motion $\Bt$.

\begin{figure}[ht]
	\centering
	\input{figures/chapter4/fig_mn.tex}
	\caption{The increments of $\Ndis$ are dependent on discrete steps of $\Bt$.} 
	\label{F: Mn}
\end{figure}

We will show that, if $||\rzntX - \WtX|| < \delta$, 
then $\Ndis(k) = \nnt(k)$ for all $k \leq \n{2}{3}T$ with high probability.
For this, we redefine both processes using a coupling argument.
\fxnote{Find out what a coupling argument is.}
At step $k$, let
\begin{equation} \label{E: def alpha beta}
\begin{aligned}
\alpha_k := \min(\zminz(k), \n{1}{3}\Bt(\n{-2}{3}k)), \\
\beta_k := \max(\zminz(k), \n{1}{3}\Bt(\n{-2}{3}k)).
\end{aligned}
\end{equation}
Now define random variables
\begin{equation} \label{E: def xi eta}
\begin{aligned}
\xi_k &\sim \Binom(\alpha_k, \p), \\
\eta_k &\sim \Binom(\beta_k - \alpha_k, \p).
\end{aligned}
\end{equation}
So $\xi_k + \eta_k \sim \Binom(\beta_k, \p)$.

\begin{figure}[ht]
	\centering
	\input{figures/chapter4/fig_bm_bfwalk.tex}
	\caption{The upscaled reflected Brownian motion and the reflected breadth-first walk define $\alpha_k$ and $\beta_k$.} 
	\label{F: BM BF-walk}
\end{figure}

Consider the two possibilities at time $k$:
Either $\zminz(k) \leq \n{1}{3}\Bt(\n{-2}{3}k)$,
then $\alpha_k = \zminz(k)$ and $\beta_k = \n{1}{3}\Bt(\n{-2}{3}k)$, 
so 
\begin{equation}
\begin{aligned}
\xi_k &=_d \nnt(k) - \nnt(k-1), \\
\xi_k + \eta_k &=_d \Ndis(k) - \Ndis(k-1).
\end{aligned}
\end{equation}
Or $\zminz(k) > \n{1}{3}\Bt(\n{-2}{3}k)$,
then $\alpha_k = \n{1}{3}\Bt(\n{-2}{3}k)$ and $\beta_k = \zminz(k)$, 
so
\begin{equation}
\begin{aligned}
\xi_k &=_d \Ndis(k) - \Ndis(k-1), \\
\xi_k + \eta_k &=_d \nnt(k) - \nnt(k-1).
\end{aligned}
\end{equation}

This way, we can define $\nnt$ and $\Ndis$ by
\begin{equation}
\nnt(k) - \nnt(k-1) = 
\begin{cases}
\xi_k & \text{if} \enspace \zminz(k) \leq \n{1}{3}\Bt(\n{-2}{3}k), \\
\xi_k + \eta_k &\text{else}
\end{cases}
\end{equation}
and
\begin{equation}
\Ndis(k) - \Ndis(k-1) = 
\begin{cases}
\xi_k + \eta_k & \text{if} \enspace \zminz(k) \leq \n{1}{3}\Bt(\n{-2}{3}k), \\
\xi_k &\text{else}.
\end{cases}
\end{equation}
By \eqref{E: def alpha beta} and \eqref{E: def xi eta},
these definitions maintain
\begin{equation*}
\begin{aligned}
\nnt(k) - \nnt(k-1) &\sim \Binom(\zminz(k), \p), \\
\Ndis(k) - \Ndis(k-1) &\sim \Binom(\n{1}{3} \Bt(\n{-2}{3}k), \p).
\end{aligned}
\end{equation*}
We see that, no matter the relation of $\zminz(k)$ and $\n{1}{3}\Bt(\n{-2}{3}k)$, 
the increments of the processes differ only by the random variable $\eta_k$.

Conditioning on $||\znt - \WtX|| < \delta$, for $\Ndis(k) \neq \nnt(k)$ to hold for some $k$,
there has to have been a step in which the increments of both processes were different.
We evaluate
\begin{equation} \label{E: Prob eta not 0}
\begin{aligned}
&\Prob(\exists k\leq \n{2}{3}T: \Ndis(k) \neq \nnt(k) \cond \Event{\delta}) \\
&\quad \leq \sum_{k=1}^{\n{2}{3}T} \Prob( \Ndis(k) - \Ndis(k-1) \neq \nnt(k) - \nnt(k-1) \cond \Event{\delta}) \\
&\quad \leq \sum_{k=1}^{\n{2}{3}T} \Prob( \eta_k \neq 0 \cond \Event{\delta}) \\
&\quad \leq \n{2}{3}T \max_{k \leq \n{2}{3}T} \Prob(\eta_k \neq 0 \cond \Event{\delta}).
\end{aligned}
\end{equation}

Since $||\znt - \WtX|| < \delta$, we know $\beta_k - \alpha_k < \delta \n{1}{3}$. Therefore
$\eta_k \leq_{\text{st.}} \zeta \sim \Binom(\delta\n{1}{3}, \p)$ for all $k \leq \n{2}{3}T$.

Using Markov's inequality gives
\begin{equation}
\Prob( \eta_k \neq 0 \cond \Event{\delta} ) \leq \Prob( \zeta_k \geq 1 ) \leq \Exp{\zeta_k} = \delta\n{1}{3}\p,
\end{equation}
and substituting in \eqref{E: Prob eta not 0} we obtain
\begin{equation}
\begin{aligned}
&\Prob(\exists k\leq \n{2}{3}T: \Ndis(k) \neq \nnt(k) \cond \Event{\delta}) \\
&\quad \leq \n{2}{3}T \max_{k \leq \n{2}{3}T} \Prob( \eta_k \neq 0 \cond \Event{\delta} ) \\
&\quad \leq \n{2}{3}T\delta\n{1}{3}\p \\
&\quad \leq n \p T \delta \\
&\quad \leq 2T\delta 
\end{aligned}
\end{equation}
for large $n$.

We now define $\rNdis(s) = \n{-1}{3} \Ndis(\n{2}{3}s)$ and continue the estimation from \eqref{E: Estimation step 3},
which yields
\begin{equation} \label{E: Estimation step 4}
\begin{aligned}
\Exp{f(\znt, \nnt)} 
&\leq \Exp{f(\WtX, \nnt) \cond \WtX, \Event{C}, \Event{\delta}}\Prob(\Event{C})\Prob(\Event{\delta}) + \epsilon + 2M\epsilon   \\
&\leq \Exp{f(\WtX, \rNdis) \cond \WtX, \Event{C}, \Event{\delta}}\Prob(\Event{C})\Prob(\Event{\delta}) + 2\delta T M + \epsilon  + 2M\epsilon  .
\end{aligned}
\end{equation}

We want to drop the conditioning on $\Event{C}$ and $\Event{\delta}$ from the expectation in \eqref{E: Estimation step 4}
and calculate
\begin{equation*}
\begin{aligned}
	\Exp{f(\WtX, \rNdis) \cond \WtX } 
	&= \Exp{f(\WtX, \rNdis) \cond \WtX, \Event{C}} \Prob(\Event{C}) \\
	&\quad + \Exp{f(\WtX, \rNdis) \cond \WtX, \neg \Event{C}} \Prob(\neg\Event{C}) \\
	&= \Exp{f(\WtX, \rNdis) \cond \WtX, \Event{C}, \Event{\delta} } \Prob(\Event{C}) \Prob(\Event{\delta}) \\
	&\quad + \Exp{f(\WtX, \rNdis) \cond \WtX, \Event{C}, \neg \Event{\delta}} \Prob(\Event{C}) \Prob(\neg \Event{\delta})\\
	&\quad + \Exp{f(\WtX, \rNdis) \cond \WtX, \neg \Event{C}} \Prob(\neg\Event{C}).
\end{aligned}
\end{equation*}

As previously established, $\Prob(\Event{C}), \Prob(\Event{\delta}) > 1 - \epsilon$ for sufficiently large $n$.
The boundedness of $f$ implies $f(\WtX, \rNdis) \geq -M$, therefore
\begin{equation*}
\begin{aligned}
	&\Exp{f(\WtX, \rNdis) \cond \WtX, \Event{C}, \Event{\delta} } \Prob(\Event{C}) \Prob(\Event{\delta}) \\
	&\qquad = \Exp{f(\WtX, \rNdis) \cond \WtX } \\
	&\qquad\quad - \Exp{f(\WtX, \rNdis) \cond \WtX, \Event{C}, \neg \Event{\delta}} \Prob(\Event{C}) \Prob(\neg \Event{\delta}) \\
	&\qquad\quad - \Exp{f(\WtX, \rNdis) \cond \WtX, \neg \Event{C}} \Prob(\neg\Event{C}) \\
	&\qquad \leq \Exp{f(\WtX, \rNdis) \cond \WtX } + 2\epsilon M.
\end{aligned}
\end{equation*}

Substituting this result in \eqref{E: Estimation step 4} gives
\begin{equation} \label{E: Estimation step 5}
\begin{aligned}
	\Exp{f(\znt, \nnt)} 
	&\leq \Exp{f(\WtX, \rNdis) \cond \WtX, \Event{C}, \Event{\delta}}\Prob(\Event{C})\Prob(\Event{\delta}) + 2\delta T M + \epsilon  + 2M\epsilon  \\
	&\leq \Exp{f(\WtX, \rNdis) \cond \WtX } + 2\delta T M + \epsilon  + 4M\epsilon.
\end{aligned}
\end{equation}

This inequality holds for all bounded, continuous functions $f$. 
Therefore it holds for $-f$ as well, which implies
\begin{equation}
\begin{aligned}
\Exp{-f(\znt, \nnt)} &\leq \Exp{-f(\WtX, \rNdis) \cond \WtX}+ 2\delta T M  + \epsilon + 4M\epsilon \\
\iff  \Exp{f(\znt, \nnt)} &\geq \Exp{f(\WtX, \rNdis) \cond \WtX} - 2\delta T M  - \epsilon - 4M\epsilon,
\end{aligned}
\end{equation}
and therefore
\begin{equation} \label{E: Equality of Expectations}
	|\Exp{f(\znt, \nnt)} - \Exp{f(\WtX, \rNdis) \cond \WtX} | \leq 2\delta T M + \epsilon  + 4M\epsilon.
\end{equation}
\end{proofpart}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proofpart 4: M ->_d N
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proofpart}
We now show that $\rNdis \rightarrow_d \Nt$.
Since $\nnt$ is tight and $\nnt$ is equal to $\Ndis$ with high probability,
$\Ndis$ and $\rNdis$ are tight and it suffices proving convergence in finite dimensional distributions,
in this case for all $0 \leq s_1 < s_2 < \dots < s_l \leq T$:
\begin{equation} \label{E: convergence fdd}
\begin{aligned}
&\Prob( \rNdis(s_1) = k_1, \dots, \rNdis(s_l) = k_l ) \\
&\quad \xrightarrow{n \rightarrow \infty}  \Prob( \Nt(s_1) = k_1, \dots, \Nt(s_l) = k_l ).
\end{aligned}
\end{equation}

Recall that $\Nt$ is a Poisson point process, continuous on $\Real$ with rate $\Bt$,
thus the increments of $\Nt$ are independent and for all $a<b$: 
\begin{equation}
\Nt(b) - \Nt(a) \sim \Poisson( \int_{a}^{b} \Bt(s)ds).
\end{equation}
In contrast, $\Ndis$ is a discrete process whose increments are defined by $\Bt$ at integer times, 
that is for all $k \leq T$: 
\begin{equation} \label{E: increments Ndis 2}
\Ndis(k) - \Ndis(k-1) \sim \Binom(\n{1}{3}\Bt(\n{-2}{3}k), \p)
\end{equation}
We can evaluate the distribution of the increments of $\rNdis$ by
\begin{equation}
\rNdis(k) - \rNdis(k-1) = \n{-1}{3}( \Ndis(\n{2}{3}k) - \Ndis(\n{2}{3}(k-1)).
\end{equation}
Between times $\n{2}{3}(k-1)$ and $\n{2}{3}k$, there are multiple integer steps,
the increment in each step as defined in \eqref{E: increments Ndis 2}. 
Thus
\begin{equation}
\rNdis(k) - \rNdis(k-1) \sim \n{-1}{3} \sum_{i=\n{2}{3}s_{j-1}+1}^{\n{2}{3}s_j} \Binom( \n{1}{3}\Bt(\n{-2}{3}i), \p ).
\end{equation}
Since the increments are independent, we can move the sum inside the argument of the Binomial distribution.
Let us define 
\begin{equation*}
R_{n,j} := \sum_{i=\n{2}{3}s_{j-1}+1}^{\n{2}{3}s_j}\n{1}{3}\Bt(\n{-2}{3}i)
\end{equation*}
and compute the probability in \eqref{E: convergence fdd} as
\begin{equation} \label{E: distribution Ndis}
\begin{aligned}
&\Prob(\rNdis(s_1) = k_1, \dots, \rNdis(s_l) = k_l) \\
&\quad = \Prob( \rNdis(s_j) - \rNdis(s_{j-1}) = k_j - k_{j-1}, \enspace \forall j=2,\dots,l ) \\
&\quad = \prod_{j=2}^{l} \Prob( \n{1}{3}\rNdis(s_j) - \n{1}{3}\rNdis(s_{j-1}) = \n{1}{3}(k_j - k_{j-1})) \\
&\quad = \prod_{j=2}^{l} \Prob( Y_{n,j} = \n{1}{3}(k_j - k_{j-1}) ),
\end{aligned}
\end{equation}
where $Y_{n,j} \sim \Binom(R_{n,j}, \p)$.
Note that $Y_{n,j} =_d \sum_{k=1}^{R_{n,j}} \xi_k$, with $\xi_k \sim \Bern(\p)$.

In the next step we use the Poisson limit theorem, see for example \cite[Theorem 3.7, p.79]{Klenke2013},
which states that for a series of binomially distributed random variables $X_k \sim \Binom(N_k, p_k)$
with $\Exp{X_k} = N_k p_k \rightarrow \lambda \in \Real$ as $k \rightarrow \infty$,
the convergence 
\begin{equation}
	X_k \rightarrow_d \Poisson(\lambda)
\end{equation}
holds as $k \rightarrow \infty$.

To apply this theorem, we calculate the expected value of $Y_{n,j}$:
\begin{align*}
\Exp{Y_{n,j}} 
&= R_{n,j} \p \\
&= \frac{1}{n} R_{n,j} + \BigO{\n{-1}{3}}\\
&= \frac{1}{n} \sum_{i=\n{2}{3}s_{j-1}+1}^{\n{2}{3}s_j}\n{1}{3}\Bt(\n{-2}{3}i) + \BigO{\n{-1}{3}}\\
&= \sum_{i=\n{2}{3}s_{j-1}+1}^{\n{2}{3}s_j}\n{-2}{3}\Bt(\n{-2}{3}i) + \BigO{\n{-1}{3}}
\end{align*}
\fxfatal{Improve Riemann Sum argument.}

This sum represents a partition of the interval $[s_{j-1}, s_j]$ into $\n{2}{3}(s_j - s_{j-1})$ subintervals, each of length $\n{-2}{3}$.
Since $\n{-2}{3}i$ is an element of its correpsonding subinterval, 
we are dealing with a Riemann sum over the continuous function $\Bt$.
Since $\Bt$ is bounded almost surely on the compact interval $[s_{j-1}, s_j]$,
the sum converges to an integral and
\begin{equation}
\Exp{Y_{n,j}} \longrightarrow \int_{s_{j-1}}^{s_j} \Bt(u)du.
\end{equation}

Now applying the Poisson limit theorem yields
\begin{equation} \label{E: convergence dist Yj}
Y_{n,j} \rightarrow_d \Poisson( \int_{s_{j-1}}^{s_j} \Bt(u)du ) =_d \Nt(s_j) - \Nt(s_{j-1}),
\end{equation}
and, applying this convergence in \eqref{E: distribution Ndis}, we arrive at
\begin{equation*}
\begin{aligned}
&\Prob(\rNdis(s_1) = k_1, \dots, \rNdis(s_l) = k_l) \\
&\quad = \prod_{j=2}^{l} \Prob( Y_{n,j} = \n{1}{3}(k_j - k_{j-1}) ) \\
&\enspace \longrightarrow \prod_{j=2}^{l} \Prob( \Nt(s_j) - \Nt(s_{j-1}) = \n{1}{3}(k_j - k_{j-1}) ) \\
&\quad = \Prob(\Nt(s_1) = k_1, \dots, \Nt(s_l) = k_l).
\end{aligned}
\end{equation*}
This proves $\rNdis \rightarrow_d \Nt$ and consequently
\begin{equation} \label{E: Convergence Ndis Nt}
	\Exp{f(\WtX, \Ndis) \cond \WtX} \longrightarrow \Exp{f(\WtX, \Nt) \cond \Wt}.
\end{equation}
Combining \eqref{E: Equality of Expectations} and \eqref{E: Convergence Ndis Nt} yields
\fxfatal{Add explanantion of Equality}
\begin{equation}
\begin{aligned}
&\Exp{f(\rzntX, \rnntX)} = \Exp{\Exp{f(\rzntX, \rnntX) \cond \WtX}} \\
&\qquad \xrightarrow{n \rightarrow \infty} \Exp{\Exp{f(\WtX, \Nt) \cond \WtX}} = \Exp{f(\WtX, \Nt)},
\end{aligned}
\end{equation}
\fxnote{Fix formatting here.}
which proves $(\rznt, \rnnt) \rightarrow_d (\Wt, \Nt)$ and completes the proof.
\end{proofpart}
\end{proof}


We can now assure ourselves that the overestimated probability \eqref{E: rate Nnt} is asymptotically negligible.
Assume the chance that any vertex encounters two or more surplus edges is non-zero and does not converge to zero as $n \rightarrow \infty$.
If a vertex connects by multiple excess edges, the process $\nnt$ makes two or more jumps during the time-interval of length $1$.
The rescaling \eqref{E: rescale Nnt} compresses the time axis until, in the limit process $\Nt$, 
any distance in an interval of original length $1$ will be reduced to a single point.
Consequently there would be a non-zero chance that the counting process has multiple coincident points.
But by definition $\Nt$ is a Poisson counting process with continuous intensity $\Bt$,
therefore it is simple with probability $1$, see \cite[Remark 2.1, p.34]{Haenggi.2013}.
We conclude that the probability of a vertex having multiple surplus edges must tend to zero.









