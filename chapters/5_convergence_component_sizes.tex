% Chapter 4: Convergence of component sizes
% Contains:
%   Explanation on what's still missing
%   The deterministic lemma (Lemma 7)
%   Girsanov Theorem
%   Applying Lemma 7 (Lemma 8)
%   Graph theory approach:
%     Lemma 9
%   l2 approach:
%     Definitions on l2
%     Deterministic lemma (Lemma 14)
%     Proposition 15
%     Applying Prop 15 to the bf-walk

\chapter{Convergence of component sizes} \label{C: convergence}
\fxnote{Update title.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% What's still missing
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In Chapter~\ref{C: bf-walk} we have shown that the rescaled breadth-first walk $\rznt$ on $\Gnt$
converges in distribution to the Brownian motion with drift $\Bt$.
Intuitively it is clear that Theorem~\ref{T: Main} should follow:
Component sizes are coded into the breadth-first walk as excursions above past minima,
excursions of $\Bt$ are excursions of $\Wt$ above past minima.
But rigorously deducing a proof of our main theorem requires a bit more work.
To make sure that indeed components and excursions do match up,
we describe them as two-dimensional point processes
in which the first entry gives the start of an excursion or component, 
the second the length of an excursion or the size of a component.
The following first two Lemmata prove that this sequence of Poisson point processes describing components 
converges to the point process describing excursions with regard to the vague topology.

For the proof of Theorem~\ref{T: Main} to hold, it remains to be shown that this encompasses all relevant components and excursions.
One problem that might arise is the starting point of a component of size greater than some $\delta \n{2}{3}$, 
which needs to be considered in the convergence, to diverge to infinity as $n \rightarrow \infty$.
Since we are working in the vague topology of counting processes, 
this would entail the component not necessarily converging in distribution to some excursion of $\Bt$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Deterministic Lemma 7: Statement and Note
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We start with a deterministic Lemma.
Given a continuous function $f$ with properties similar to a Brownian motion
and a sequence of functions $f_n$ converging uniformly to $f$,
we can define a set of excursions on each $f_n$ such that
the point processes of starts and lengths of excursions converge vaguely to the point processes of starts and lengths of excursions on $f$.

\begin{lemma} \label{L: Deterministic Lemma}
	Let $f:[0, \infty) \rightarrow \Real$ be a continuous function. 
	Let $\mathcal{E}$ be the set of non-empty intervals 
	$e=[l,r] \subset \Real_{\geq 0}$
	such that
	\begin{equation} \label{E: f cond 1}
	f(r) = f(l) = \min_{s \leq l} f(s),
	\end{equation}
	\begin{equation} \label{E: f cond 2}
	f(l) < f(s) \quad \forall l < s < r.
	\end{equation}
	Define $\Xi := \lbrace (l, r-l) \; | \; (l, r) \in \mathcal{E} \rbrace$.
	
	Suppose that for intervals $(l_1, r_1), (l_2, r_2) \in \mathcal{E}$ with $l_1 < l_2$ we have 
	\begin{equation} \label{E: f cond f(l1) > f(l2)}
	f(l_1) > f(l_2)
	\end{equation}
	and the complement of $\cup_{e \in \mathcal{E}} (l,r)$ has Lebesgue measure zero,
	\begin{equation} \label{E: f cond complement zero}
	\mu \left( \left( \cup_{e \in \mathcal{E}} (l,r) \right)^c\right) = 0.
	\end{equation}
	
	Let $f_n \rightarrow f$ as $n \rightarrow \infty$ uniformly on bounded intervals.
	Now define $\Xin := \lbrace (\tn{i}, \tn{i+1} - \tn{i}) \; | \; i \geq 1 \rbrace$
	for any sequence of sets of points $\fnPoints := (\tn{i}, i \geq 1)$ satisfying the following conditions:
	
	\begin{equation} \label{E: fn cond 3}
	0 = \tn{1} < \tn{2} < ... \; \text{and} \; \lim_{i \rightarrow \infty} \tn{i} = \infty,
	\end{equation}
	\begin{equation} \label{E: fn cond 4}
	f_n(\tn{i}) = \min_{u \leq \tn{i}}f_n(u), 
	\end{equation}
	\begin{equation} \label{E: fn cond 5}
	\max_{i: \tn{i} \leq s_0}(f_n(\tn{i}) - f_n(\tn{i+1})) \rightarrow 0 \; \text{as} \; n \rightarrow \infty, \; \text{for all} \; s_0 \leq \infty.
	\end{equation}
	
	Then $\Xin \rightarrow_v \Xi$ as $n \rightarrow \infty$.
\end{lemma}

\begin{note}
	The convergence
	$\Xin \rightarrow_v \Xi$
	is to be interpreted as the vague convergence of counting measures,
	which means that for all continuous functions
	$f: \IntTimesInt \rightarrow \Real$
	with compact support,
	\begin{equation}
		\Exp{f(\Xin)} \rightarrow \Exp{f(\Xi)}.
	\end{equation}
	By general theory,
	see for example \cite{Kallenberg1990},
	\fxnote{Add real reference here}
	this is equivalent to the convergence
	\begin{equation}
	\Xin(C) \rightarrow \Xi(C)
	\end{equation}
	for all compact subsets of
	$\IntTimesInt$,
	where $\Xi(\partial C) = 0$.
	In this case, a compact subset is a pair of closed intervals
	$[T_1, T_2] \times [d_1, d_2]$,
	with $T_1, T_2 \geq 0$
	and $d_1, d_2 > 0$.
	The condition on the measure of the boundary means,
	that the limit process must not have any excursions starting exactly at
	$T_1$ or $T_2$,
	or any excursions of length exactly $d_1$ or $d_2$.
	An exception is the case of $T_1=0$.
	Since the domain of $f$ starts at $0$,
	and the boundary condition is needed to prevent the case of points in $\Xin$ converging to some point on the boundary "from outside",
	\fxnote{This is not really a boundary condition.}
	\fxnote{Are "" around "from outside" needed?}
	\fxnote{This is not a nice sentence in general.}
	we do not need to consider $0$ as part of the boundary of any interval 
	$[0, T_2] \times [d_1, d_2]$.
\end{note}
\fxnote{Does this fit in the note? 
	General stuff about vague convergence should not be in the proof.
	Maybe vague convergence before the Lemma, interval stuff in the note.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Deterministic Lemma 7: Proof
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
%	Structure:
%	- What does the definition of \Xi and \Xin mean?
%	- Detail on \Xin and conditions
%   First part: \Xi in \Xin
%   (For every f exc, there is an fn exc)
%	- Facts about f:
%	 - It goes up to the left and right of l
%	 - It goes down to the right of r? (Or later)
%	 - eps/delta stuff
%	- Arch of fn
%	- Definition ln(x)
%	- ln(x) in [l-e, l+e]
%	- Definition rn(x)
%	- rn(x) in [r-e, r+e]
We begin by elaborating what the conditions
\eqref{E: f cond 1} to \eqref{E: fn cond 5}
mean for the points in
$\Xi$ and $\Xin$.
\fxfatal{Some stuff about excursions goes here.}

To prove the Lemma, 
we fix some bounded subset of
$\IntTimesInt$,
$C := [T_1, T_2] \times [d_1, d_2]$,
and show that, for sufficiently large $n$,
$\Xin(C) = \Xi(C)$,
that is,
there are exactly as many excursions of $f_n$ starting in
$(T_1, T_2)$, 
with length in $(d_1, d_2)$,
as similar excursions of the limit function $f$.

We will first show that every excursion of $f$ is eventually matched by some excursion of $f_n$,
then show that there can not be any more excursion of $f_n$ of sufficient length.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proof Part 1: \Xi in \Xin
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proofpart}[$\Xin(C) \subseteq \Xi(C)$] \label{PP: Lemma 7 1}

Let us first establish some facts about excursions of the limit function $f$.
In the interval $[T_1, T_2]$,
there can only be a finite number of excursion starting with length of at least $d_1$,
at most $T_1/d_1$.
We call excursions of length greater than $d_1$ large, all other excursions small.
Let
$ \mathcal{E}^* := \{ (l_i, r_i) \; | \; i=1, \dots, k \} $
be the set of these excursions.
Consider a fix
$(l,r) \in \mathcal{E}^*$.
\fxnote{Some figure showing an excursion goes here probably.}
Since $l-r > d_1$,
we can find an $\epsilon > 0$
such that the length of the interval
$[l+\epsilon, r-\epsilon]$
is still greater than $d_1$.

We can also find an $\epsilon > 0$ sufficiently small,
so that for every
$x \in [0, l-\epsilon] \cup [l+\epsilon, r-\epsilon]$,
\begin{equation} \label{E: fx > fl + delta}
f(x) > f(l) + \delta
\end{equation}
holds for some $\delta > 0$.

Assume this does not hold to the left of the $\epsilon$-neighbourhood of $l$.
Then there exists some $x \in [0, l-\epsilon]$
for which $f(x) \leq f(l) + \delta$ for every $\delta > 0$.
This implies $f(x) \leq f(l)$,
and by condition \eqref{E: f cond 1},
$f(l) = \min_{u \leq l}f(u)$,
so $f(x) = f(l)$.
That leaves two possibilities:
First, $f$ is constant on the interval $[x,l]$.
But this would be an interval of non-zero length without any excursions on it,
a contradiction to \eqref{E: f cond complement zero}.
Second, there is an interval
$[x', l']$, with $x\leq x'<l' \leq l$,
such that $f(y) > f(x)$ for all $y \in (x',l')$.
That makes $(x',l')$ another excursion in $\mathcal{E}$,
but $f(x') = f(l)$,
which is a contradiction to condition \eqref{E: f cond f(l1) > f(l2)}.

Using the same logic,
assuming \eqref{E: fx > fl + delta} does not hold in $[l + \epsilon, r- \epsilon]$
leads to a point $x \in [l + \epsilon, r- \epsilon]$
such that $f(x) \leq f(l)$,
which is a contradiction to condition \eqref{E: f cond 2}.

Now consider the behaviour of $f$ on $[r, r + \epsilon]$.
As previously stated, 
condition \eqref{E: f cond complement zero} prevents $f$ from being constant.
If $f(x) \geq f(r)$ for all $x \in [r, r + \epsilon]$,
there would be another excursion that contradicts \eqref{E: f cond f(l1) > f(l2)}.
So there must exist an $r^* \in (r, r + \epsilon]$ with $f(r^*) < f(r)$.
Let $\delta^* := \min\{ \delta, f(r) - f(r^*) \}$,
where $\delta$ is the constant used in the discussion of
$[0, l-\epsilon]$ and $[l+\epsilon, r-\epsilon]$ above.
\fxnote{delta used in the discussion above is not nice.}


We now take a look at $f_n$.
Fix an $x^* \in [l+\epsilon, r-\epsilon]$.
We will now show that there exist points
$\lnx \in [l-\epsilon, l+\epsilon]$
and 
$\rnx \in [r-\epsilon, r+\epsilon]$
for which condition \eqref{E: fn cond 4} hold,
making $(\lnx, \rnx- \lnx)$ a possible element of $\Xin$.

Define $\lnx := \min(\argmin_{u \leq x} f_n(u))$.
\fxerror{Make ln(x) := sup(argmin(...))}
By the uniform convergence $f_n \rightarrow f$,
we can find an $N \in \Nat$,
such that $|f_n(x) - f(x)| < \delta^* / 3$ for all
$x \in [T_1, T_2]$ and $n \geq N$.
So for every point $x \in [0, l - \epsilon]\cup[l + \epsilon, r - \epsilon]$,
\begin{equation} \label{E: fn(x) > fn(l)}
f_n(x) > f(x) - \delta^* / 3 > f(l) + \delta^* - \delta^* / 3 > f(l) + \delta^* / 3 > f_n(l).
\end{equation}
\fxnote{Align these.}
So $f_n$ takes its minimum over 
$[0, l - \epsilon]\cup[l + \epsilon, r - \epsilon]$
at $l$ or somewhere in $[l-\epsilon, l+\epsilon]$.
\fxnote{Maybe give the interval a name.}
Therefore $\lnx \in [l-\epsilon, l+\epsilon]$ and, 
since we chose $\lnx$ as the smallest value for which $f_n$ attains this minimum, $\lnx = \min_{u \leq \lnx} f_n(u)$.

Now define 
$\rnx := \inf \{ x > x^* \; | \; f_n(x) = \lnx \}$.
By \eqref{E: fn(x) > fn(l)}, the function $f_n$ can not reach its past minimum $\lnx$
before $r-\epsilon$.
As previously discussed,
there exists a $r^* \in (r, r+\epsilon]$ such that $f(r) - f(r^*) \geq \delta^*$.
Now
\begin{equation}
l_n(x) > l(x) - \delta^* / 3 = f(r) - \delta^* / 3 \geq f(r^*) + \delta^* - \delta^* / 3 > f_n(r^*),
\end{equation}
\fxnote{Align these.}
which implies that $\rnx$ must be smaller than $r^*$,
thus $\rnx \in [r - \epsilon, r+ \epsilon]$.
Since $l_n(x) = \min_{u \leq \lnx} f_n(u)$
and $\rnx$ is the first time this previous minimum is reached,
$\rnx = \min_{u \leq \lnx} f_n(u)$.


We have shown that the only points satisfying condition \eqref{E: fn cond 4} must lay near the beginning and end of excursions of $f$.
This is not sufficient as proof of the Lemma, since the sequence $\fnPoints$ might not contain any points between two large excursions,
or even not contain any points, thus skipping one or more eligible excursions.
We will now show that \eqref{E: fn cond 3} and \eqref{E: fn cond 5} imply 
that any set satisfying these two conditions must contain at least one element in between two large excursions of $f$.

First of all, \eqref{E: fn cond 3} ensures there must exist points in $\fnPoints$ and no last element of $\fnPoints$ exists.

Consider two consecutive large excursions of $f$, $(l_1, r_1)$ and $(l_2, r_2)$
and the space between $r_1$ and $l_2$.
Suppose there is no element of $\fnPoints$ in $[r_1-\epsilon, l_2+\epsilon]$ for all $\epsilon>0$.
The latest element of $\fnPoints$ was located at or before $l_1$, the next will be at or around $r_2$.
We know $f(r_1)=f(l_1) > f(l_2)$, so there is $\delta>0$ such that for all large $n$,
$|f_n(l^*) - f_n(r^*)|> \delta$ for all $l^*$ and $r^*$ in the sufficiently small $\epsilon$-neighbourhoods around $l_1$ and $r_2$.

Any previous element of $\fnPoints$ will only yield a larger, any element after $r_2$ only a smaller $f_n$-value.
This is a contradiction to \eqref{E: fn cond 5}.
So there must be at least one point of $\fnPoints$ in between these two large excursions, no excursion can be skipped.

For any excursion $(l,r)$ of $f$, there exists $\lnx, \rnx \in \fnPoints$ in the respective $\epsilon$-neighbourhoods,
so the excursion of $f$ is matched by an excursion of $f_n$ of equal or greater length.
Since $l - r + 2\epsilon > d_1$,
we have found $\tn{i} = \lnx$, $\tn{i+1} = \rnx$,
such that $(\tn{i}, \tn{i+1} - \tn{i}) \in \Xin \cap C$
for all $n$ greater than some $N_i = N \in \Nat$.
Now let $N^* := \max\{N_1, \dots, N_k\}$ 
and every excursion of $f$ in $C$ is matched by an excursion of $f_n$ in $C$,
with $n \geq N^*$.
Therefore eventually $\Xi (C) \leq \Xin(C)$.
\end{proofpart}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proof Part 2: \Xin in \Xi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proofpart}[$\Xi(C) \subseteq \Xin(C)$] \label{PP: Lemma 7 2}

Now that every excursion of $f$ is matched, 
we need to show that there can not exist any additional large excursion of $f_n$.
Considering the fact that excursions can not overlap,
the only possibility for an additional large excursion is the space between two large excursions.
For a pair of large excursions,
$(l_i, r_i)$ and $(l_{i+1}, r_{i+1})$,
there is only enough space in between them if
$r_i - l_{i+1} > d_1$.
Consider such an interval $[r,l]$ of length greater than $d_1$.
By condition \eqref{E: f cond complement zero} the space must be filled with smaller excursions of $f$.
There is an at most countable number of these,
\fxnote{Why?}
let $\mathcal{E^*} := \{ (l_i, r_i) \; | \; i \in \Nat \}$
be the set of such excursions starting in $(r,l)$ with length less than or equal to $d_1$.
We know
\begin{equation} 
\sum_{i=1}^{\infty} r_i - l_i = l - r > d_1,
\end{equation}
so we can choose a finite set of excursions
$\{ (l_i, r_i) \; | \; i = 1, \dots, K \}$
such that, 
if we exclude these from the interval $[r,l]$, 
the space remaining is less than $d_1$:
\begin{equation}
l-r - \sum_{i=1}^{K} r_i - l_i < d_1.
\end{equation}
Let $d^* < \min \{ r_i - l_i \; | \; i = 1, \dots, K \}$
and apply the logic of Part~\ref{PP: Lemma 7 1} to the compact set
$[r, l] \times [d^*, d_1]$.
For sufficiently large $n$, 
every one of these $K$ excursions of $f$ will be matched with an excursion of $f_n$,
so that there will be no space left for a large excursion of $f_n$ in $[r,l]$.
Applying this logic to every one if the finitely many gaps between excursions of $f$,
we see that there can not exist any more large excursions of $f_n$  than those already matching $f$.
Thus $\Xin(C) \leq \Xi(C)$ for sufficiently large $n$,
which completes the proof.
\end{proofpart}

\end{proof}

Before applying this Lemma to our processes, we need to briefly introduce Girsanovs Theorem,
which will enable us to prove certain properties of the Brownian motion with drift.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Girsanov Theorem
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We state the theorem as in \cite[Theorem 4.2.2, p.66]{Lamberton1996}.
\begin{theorem}[Girsanov] \label{T: Girsanov}
	Let $(W(s))_{0 \leq s \leq T}$ be a Brownian motion on a probability space $(\Omega, \mathcal{F}, \mathbb{P})$.
	Let $(\Theta(s))_{0 \leq s \leq T}$ be an adapted process satisfying
	\begin{equation} \label{E: Girsanov cond Theta}
	\int_{0}^{T} \Theta^2(u)du < \infty.
	\end{equation}
	Define
	\begin{align}
		\tilde{W}(s) &:= W(s) + \int_0^s \Theta(u)du, \label{E: Girsanov def W tilde} \\ 
		X(s) &:= \exp \left\{ -\int_{0}^{s} \Theta(u) dW(u) - \frac{1}{2} \int_0^s \Theta^2(u)du \right\}. \label{E: Girsanov def X}
	\end{align}
	If $X(s)$ a martingale, that is $\Exp{X(s)} = \Exp{X(0)} = 1$ for all $s$,
	the measure $\mathbb{Q}$, defined by
	\begin{equation} \label{E: Girsanov def P tilde}
	\tilde{\mathbb{P}}(A) := \int_A X(\omega) d\mathbb{P}(\omega), \; \text{for all} \; A \in \mathcal{F},
	\end{equation}
	is a probability measure under which the process 
	$(\tilde{W}(s))_{0 \leq s \leq T}$
	is a Brownian motion.
\end{theorem}

By \cite[Remark 4.2.3, p.66]{Lamberton.2000}, a sufficient condition for $X(t)$ to be a martingale is the so-called Novikov-condition
\begin{equation} \label{E: Novikov}
\ExpBig{ \exp \left( \frac{1}{2} \int_{0}^{T} \Theta^2(u)du \right) } < \infty.
\end{equation}

We can apply this Theorem to the Brownian motion with drift $W^t$ as follows:
Recall the definition
\begin{equation}
\Wt(s) = W(s) + ts - \frac{1}{2} s^2 = W(s) + \int_0^s (t-u)du.
\end{equation}
For $T<\infty$, $\Theta(u) := t-u$ satisfies \eqref{E: Novikov},
therefore $X(t)$, as defined in \eqref{E: Girsanov def X}, is a martingale and
$\Wt$ is a standard Brownian motion under the probability measure $\tilde{\mathbb{P}}$ defined in \eqref{E: Girsanov def P tilde}.
Since $X(s) > 0$ almost surely for all $s$, the probability measures $\mathbb{P}$ and $\tilde{\mathbb{P}}$ agree which sets have probability zero:
\begin{equation} \label{E: P0 = Q0}
\mathbb{P}(A) = 0 \iff \tilde{\mathbb{P}}(A) = 0, \; \text{for all} \; A \in \mathcal{F}.
\end{equation}
Properties holding with probability $1$ or $0$ for a standard Brownian motion will hold with probability $1$ or $0$, respectively,
for $\Wt$ under $\tilde{\mathbb{P}}$ and hence under the original measure $\mathbb{P}$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Lemma 8: Statement
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The following Lemma will now link excursions of $Z_n$ and $\Wt$ in the language of Lemma~\ref{L: Deterministic Lemma}.
We define a random point process containing the starts and lengths of excursions of $\Bt$ together with a sequence of random point processes describing the starts and sizes of components discovered by $\rznt$.
Let $\Ci{n,i}$ be the size of the $i$-th component and
define $\gamma_n(i) \in \{1, \dots, n\}$ as the index for which
$v(\gamma_n(i))$ is the last vertex of the $i-1$-th component encountered by the breadth-first walk $Z_n$.
For a rigorous treatment of the following Lemma, we need to extend this definition beyond the $n$ possible values for $\gamma_n(i)$.
Let $i^* \in \Nat$ such that $\gamma_n(i^*) = n$. For all $j > i^*$, define $\gamma_n(j) := \gamma_n(j-1) + 1$.
Recall that we extended the definition of $\rznt$ in chapter~\ref{C: bf-walk} to a constant process after $s=\n{1}{3}$.

\begin{lemma} \label{L: Lemma 8}
	Let $\Xi$ be the point process with points corresponding to excursions of $\Bt$,
	\begin{equation} \label{E: Lemma 8 def Xi}
	\Xi := \{ (l(\gamma), |\gamma|) \; | \; \gamma \; \text{excursion of} \; \Bt \}.
	\end{equation}
	Let $\Xin$ be the rescaled point process with points corresponding to excursions of the breadth-first walk
	\begin{equation} \label{E: Lemma 8 def Xin}
	\Xin := \{ ( \n{-2}{3} \gamma_n(i), \n{-2}{3} \Ci{n,i} ) \; | \; i \geq 1 \}.
	\end{equation}
	Then $\Xin \rightarrow_v \Xi$ as $n \rightarrow \infty$.
\end{lemma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Lemma 8: Proof
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
	First, let us give some remarks on the structure of the objects in use here and their convergence.
	Lemma~\ref{L: Deterministic Lemma} states that, under certain conditions, two deterministic point processes converge in the vague topology.
	This Lemma now introduces $\Xin$ and $\Xi$, which are random variables mapping into the space of point processes, again equipped with the vague topology.
	
	Both $\Xin$ and $\Xi$ are random in the sense that they depend on their underlying processes, $\rznt$ and $\Wt$ respectively,
	so to clarify we define the sets
	\begin{equation} \label{E: Lemma 8 def XiWt}
	\begin{aligned}
	\Xi_{\Wt} &:= \{ (l(\gamma), |\gamma|) \; | \; \gamma \; \text{excursion of} \; \Bt \}, \\
	\Xin_{\rznt} &:= \{ ( \n{-2}{3} \gamma_n(i), \n{-2}{3} \Ci{n,i} ) \; | \; i \geq 1 \},
	\end{aligned}
	\end{equation}
	which are dependent on the specific realisation of the random processes $\Wt$ and $\rznt$.
	
	Now \eqref{E: Lemma 8 def Xi} and \eqref{E: Lemma 8 def Xin} can be redefined as the random variables
	\begin{equation} \label{E: Lemma 8 def Xi Xin}
	\begin{aligned}
	\Xi : \; &\omega \mapsto \Xi_{\Wt(\omega)}, \\
	\Xin : \; &\omega \mapsto \Xin_{\rznt(\omega)}.
	\end{aligned}
	\end{equation}
	
	We apply the reasoning used in the proof of Theorem~\ref{T: Joint Convergence} once more,
	and use the Skorohod representation theorem to construct random variables
	$\rzntX$ and $\WtX$ on a probability space $(\Omega', \mathcal{F}', \mathbb{P}')$,
	which converge almost surely.
	Almost sure convergence implies
	\begin{equation}
	\sup_{s \leq s_0}|\rzntX(s) - \WtX(s)| \rightarrow 0
	\end{equation}
	almost surely,
	so $\rzntX(\omega') \rightarrow \WtX(\omega')$ uniformly for almost all $\omega' \in \Omega'$. 
	Analogously we define
	\begin{equation}
		\BtX(s) := \WtX(s) - \min_{u \leq s}\WtX(s).
	\end{equation}
	
	On the same probability space, we now define $\XinX$ and $\XiX$ as in \eqref{E: Lemma 8 def XiWt} and \eqref{E: Lemma 8 def Xi Xin}.
	Since $\rznt \sim \rzntX$ and $\Wt \sim \WtX$ we have $\Xin \sim \XinX$ and $\Xi \sim \XiX$.
	
	By the definition of $\BtX$, 
	$\XiX$ is the $\Xi$ in Lemma~\ref{L: Deterministic Lemma} with $f = \WtX$.
	As $f_n$ we take $\rznt$ and define $\tn{i} = \n{-2}{3} \gamma(n,i)$, that is, 
	the elements of $\fnPoints$ to be the end-points of components,
	rescaled to match $\rzntX$.
	This way $\XinX$ coincides with $\Xin$ in Lemma~\ref{L: Deterministic Lemma}.
	
	We still need to show that conditions \eqref{E: f cond 1} to \eqref{E: f cond complement zero} hold for $\WtX$
	and conditions \eqref{E: fn cond 3} to \eqref{E: fn cond 5} hold for the breadth-first walk.
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	% Conditions on BM
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	We start with the former. 
	We first define the set $\mathcal{E}$ for the Brownian motion.
	Consider the set of positive rational numbers.
	For every $q \in \Rat^+$, 
	define 
	\begin{equation}
		l(q) := \sup\{ \argmin_{s < q} \WtX(s) \}
	\end{equation}
	and
	\begin{equation}
		r(q) := \inf\{ {s \in \Real \; | \; \WtX(s) = l(q)} \}.
	\end{equation}
	
	Now every rational number belongs to one excursion $(l(q), r(q))$,
	while one excursion contains multiple rational numbers.
	We define the set of excursions
	\begin{equation}
	\mathcal{E} := \bigcup_{q \in \Rat^+} \{ (l(q), r(q)) \}
	\end{equation}
	and note that it is countable.
	
	The following properties will be proven on a standard Brownian motion $W$,
	we later use Girsanovs theorem to apply them to $\WtX$.
	
	By, for example, \cite[Theorem 1.22]{Morters.2010},
	$W$ is not monotonous on any interval $[a,b]$ with $0 < a < b < \infty$ almost surely.
	
	Consider two excursions $(l_1, r_2), (l_2, r_2)$ with $l_1 < l_2$.
	If $W(l_2) = \min_{s \leq l_2} W(s) \geq f(l_1)$ then $W(s) \geq W(l_1)$ for all $l_1 < s < l_2$.
	Therefore the must exist $\epsilon>0$ such that $W$ is monotonously increasing on $[l_1, l_1 + \epsilon]$.
	For each $(l,r) \in \mathcal{E}$ this is not possible almost surely
	and since $\mathcal{E}$ is countable,
	condition \eqref{E: f cond f(l1) > f(l2)} holds with probability $1$.
	
	The complement of all excursion is the set of intervals
	on which the Brownian motion is monotonously decreasing.
	By the same reasoning, almost surely there is no interval in between any two excursions on which $W$ is monotonously decreasing,
	therefore condition{E: f cond complement zero} holds almost surely.
	
	A standard Brownian motion $W$ satisfies the conditions of Lemma~ref{L: Deterministic Lemma} almost surely,
	therefore $\WtX$ does so almost surely under $\tilde{\mathbb{P}}$
	and by Girsanovs theorem likewise under $\Prob$.  

	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	% Conditions on Zn
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	We now show that conditions 
	\eqref{E: fn cond 3} to \eqref{E: fn cond 5}
	hold for the random walk $Z_n$ and $t_{n,i} = \n{-2}{3} \gamma_n(i)$.
	
	The breadth-first walk $\znt$ is a discrete process, therefore for all $n \in \Nat$ and $i \geq 2$:
	\begin{equation}
		\tn{i} - \tn{i-1} = \n{-2}{3}\gamma_n(i) - \n{-2}{3}\gamma_n(i-1) \geq \n{-2}{3} > 0.
	\end{equation}
	\fxnote{fix scaling?}
	By definition of $\gamma_n(i)$,
	$\lim_{i \rightarrow \infty} \gamma_n(i) = \infty$ for all $n$
	which establishes condition \eqref{E: fn cond 3}.
	
	The breadth-first walk attains a new minimum at the end of every component,
	which ensures \eqref{E: fn cond 4}.
	The difference between the levels of $\znt$ at the end of two consecutive components is always $1$,
	so 
	\begin{equation}
		\max_{i: \tn{i} \leq s_0}(\rzntX(\tn{i}) - \rzntX(\tn{i+1})) = \n{-1}{3} \xrightarrow{n \rightarrow \infty} 0
	\end{equation}
	for all $s_0 > 0$.
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	% Completing the proof
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	For a given realisation of $\rzntX(\omega')$ and $\WtX(\omega')$ 
	the processes and sets defined meet all conditions of Lemma~\ref{L: Deterministic Lemma}
	and we can establish the convergence
	\begin{equation}	
		\XinX_{\rzntX(\omega')} \rightarrow_v \XiX_{\WtX(\omega')}.
	\end{equation}
	This convergence holds almost surely, so
	\begin{equation}
		\XinX \rightarrow_{a.s.} \XiX
	\end{equation}
	with regard to the vague topology.
	Since almost sure convergence implies convergence in distribution we have
	\begin{equation}
		\XinX \rightarrow_d \XiX
	\end{equation}
	and therefore
	\begin{equation}
		\Xin \rightarrow_d \Xi
	\end{equation}
	with regard to the vague topology, which completes the proof.	
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% What's still missing
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

It remains to be shown that we need not consider the problem of a large component wandering off to infinity.
We prove this using to approaches:
On one hand, we can analyse the behaviour of the breadth-first walk $\znt$
after a certain stopping time.
On the other hand, 
we can consider the behaviour of components of the random graph
and the distribution of vertices on them.
\fxnote{This whole paragraph is a placeholder for something nicer.}

Consider the functions
\begin{align}
T(y) &:= \min \{s \;|\; \Wt(s)=-y\}, \label{E: def T(y)}, \\
T_n(y) &:= \min\{i \;|\; \Zn{i}=-\floor{y\n{1}{3}} \}. \label{E: def Tn(y)}.
\end{align}
Once the process $\Zn$ reaches step $T_n(y)$, 
the walk has traversed at least all vertices labeled $\{1,2,\dots,\floor{y\n{1}{3}}\}$,
since it encountered $\floor{y\n{1}{3}}$ individual components.
\fxnote{More explanation of this process. Maybe write in German then translate.}
By Theorem~\ref{T: Z -> W}, $\n{-2}{3}T_n(y) \rightarrow_d T(y)$
and since $T(y) \rightarrow \infty$ as $y\rightarrow\infty$,
$T_n(y) \approx \n{2}{3}T(y) \rightarrow\infty$ as $y\rightarrow \infty$.
This implies that we will not expect the walk to eventually discover only one giant component,
but many smaller ones that eventually end.

\section{Late excursions of $Z_n$}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: Late excursions of Z_n
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We now try to provide an exact depiction of the considerations at the end of the last section.
The following Lemma proves that with high probability, all relevant excursions do in fact take place in a compact subset of $\IntTimesInt$.

\begin{lemma} \label{L: late excursions}
	Let 
	$\Prob(\ExcursionEvent)$ 
	be the probability that the breadth-first walk encounters an excursion $\gamma$ of length 
	$|\gamma| > \delta\n{2}{3}$, starting after step $C\n{2}{3}$.
	Then for all $\epsilon>0$ exists $C>0$ such that $\Prob(\ExcursionEvent) < \epsilon$. 
\end{lemma}

\begin{proof}
	By the law of total expectation,
	\begin{equation} \label{E: P(ExcEvent) 1}
	\begin{aligned}
	\Prob(\ExcursionEvent) 
	&\leq \Exp{ \text{Number of excursions $\gamma$ with $|\gamma| \geq C\n{2}{3}$ and $l(\gamma) \geq \delta\n{2}{3}$ } } \\
	&= \Exp{ \sum_{\gamma: \; l(\gamma) \geq \delta\n{2}{3}} \Ind{\{|\gamma| \geq C\n{2}{3} \} } } \\
	&\leq \Exp{ \sum_{\gamma: \; l(\gamma) \geq \delta\n{2}{3}} \frac{|\gamma|^2}{\delta^2\n{4}{3}} \Ind{\{|\gamma| \geq C\n{2}{3} \} }} \\
	&= \frac{1}{\delta^2\n{4}{3}} \Exp{ \sum_{\gamma: \; l(\gamma) \geq \delta\n{2}{3}}  |\gamma|^2 \Ind{\{|\gamma| \geq C\n{2}{3} \} }}.
	\end{aligned}
	\end{equation}
	
	Let $T$ be the time the last excursion starting before $C\n{2}{3}$ ends.
	\fxnote{Maybe diagram goes here.}
	The behaviour of the breadth-first walk after $T$ will be the same as the behaviour of a new walk on $\Gcal(n-T,p)$.
	\fxnote{Some more explanation on this, please!}
	We write $\Ccal \in \Gcal$ to denote a component $\Ccal$ contained in the random graph $\Gcal$,
	and $|\Ccal|$ for its size.
	Since the notions of excursions of the breadth-first walk and components in the underlying graph are interchangeable,
	we can rewrite \eqref{E: P(ExcEvent) 1} as
	\begin{equation} \label{E: P(ExcEvent) 2}
	\begin{aligned}
	\Prob(\ExcursionEvent) 
	&\leq \frac{1}{\delta^2\n{4}{3}} \Exp{ \sum_{\Ccal \in \Gcal(n-T, p)}  |\Ccal|^2 \Ind{\{|\Ccal| \geq \delta\n{2}{3} \} }} \\
	&\leq \frac{1}{\delta^2\n{4}{3}} \Exp{ \sum_{\Ccal \in \Gcal(n-C\n{2}{3}, p)}  |\Ccal|^2 \Ind{\{|\Ccal| \geq \delta\n{2}{3} \} }} \\
	&\leq \frac{1}{\delta^2\n{4}{3}} \Exp{ \sum_{\Ccal \in \Gcal(n-C\n{2}{3}, p)}  |\Ccal|^2 }.
	\end{aligned}
	\end{equation}
	
	For ease of notation we consider the graph $\Gcal(k,p)$ and calculate
	\begin{equation} \label{E: P(ExcEvent) 3}
	\begin{aligned}
	\Exp{\sum_{\Ccal \in \Gcal(k, p)} |\Ccal|^2 } 
	&= \Exp{ \sum_{\Ccal \in \Gcal(k, p)} |\Ccal| \sum_{v \in \Ccal} 1} \\
	&= \Exp{ \sum_{\Ccal \in \Gcal(k, p)} |\Ccal| \sum_{v \in \Gcal(k,p)} \Ind{\{v \in \Ccal\}} } \\
	&= \sum_{v \in \Gcal(k,p)} \Exp{ \sum_{\Ccal \in \Gcal(k, p)} |\Ccal| \Ind{\{v \in \Ccal\}} } \\ 
	&= \sum_{v \in \Gcal(k,p)} \Exp{|\Ccal(v)|} \\
	&= k \Exp{|\Ccal(1)|},
	\end{aligned}
	\end{equation}
	where $\Ccal(v)$ denotes the component containing the vertex $v$ and the last inequality stems from the interchangeability of the vertex labels.
	We will bound the expectation of the size of this component from above by a suitable branching process $(Y_i, \; i\geq 0)$.
	Starting at time $0$ with one vertex, we have $Y_0 = 1$.
	The number of children of this vertex is a $\Binom(k-1,p)$ distributed random variable, $Y_1$.
	In the next step, each child-vertex will itself have children,
	each Binomially distributed on the remaining set of vertices with probability $p$.
	We compute 
	\begin{equation}
	\begin{aligned}
	Y_{2,1} &\sim \Binom(k-1-Y_1,p), \\ 
	Y_{2,2} &\sim \Binom(k-1-Y_1-Y_{2,1},p), \\
	&\dots \\
	Y_{2,Y_1} &\sim \Binom(k-1-Y_1-Y_{2,1}-\dots-Y_{2,Y_1-1},p) \\
	Y_2 &= \sum_{i=1}^{Y_1} Y_{2,i}.
	\end{aligned}
	\end{equation}
	The size of the component will then be the total amount of explored vertices,
	which is the sum of all $Y_j$, $j \geq 0$.
	To provide an upper bound we consider the branching process where each amount of children is $\Binom(k,p)$ distributed.
	Define the process as follows,
	\begin{equation}
	\begin{aligned}
	Z_0 &:= 1, \\
	Z_j &:= \sum_{i=1}^{Z_{j-1}} Z_{j,i},
	\end{aligned}
	\end{equation}
	where $Z_{j,i} \sim\Binom(k,p)$ for all $i,j \geq 1$.
	Then the process $(Y_i, \; i\geq 0)$ is stochastically dominated by 
	$(Z_i, \; i\geq 0)$ and
	\begin{equation}
	|\Ccal(1)| \leq_{\text{st.}} Z_0 + Z_1 + Z_2 + \dots
	\end{equation}
	which gives
	\begin{equation} \label{E: Exp C(1) <= sum Z}
		\Exp{|\Ccal(1)|} \leq \sum_{j=0}^{\infty}\Exp{Z_j}.
	\end{equation}	
	For $j\geq0$ we calculate the expectation of $Z_j$ by
	\begin{equation}
	\begin{aligned}
	\Exp{Z_j} 
	&= \Exp{Z_{j-1}}kp \\
	&\dots \\
	&= \Exp{Z_0}(kp)^j \\
	&= (kp)^j.
	\end{aligned}		
	\end{equation}
	Substituting in \eqref{E: Exp C(1) <= sum Z} gives
	\begin{equation} \label{E: Exp C(1) <= frac kp}
	\begin{aligned}
	\Exp{|\Ccal(1)|} 
	&\leq \sum_{j=0}^{\infty} (kp)^j \\
	&= \frac{1}{1-kp}.
	\end{aligned}		
	\end{equation}
	We continue the calculation in \eqref{E: P(ExcEvent) 2} using \eqref{E: P(ExcEvent) 3} and \eqref{E: Exp C(1) <= frac kp},
	which yields
	\begin{equation}
	\begin{aligned}
	\Prob(\ExcursionEvent) 
	&\leq \frac{n-C\n{2}{3}}{\delta^2 \n{4}{3}} \frac{1}{1-(n-C\n{2}{3})p} \\
	&= \frac{n-C\n{2}{3}}{\delta^2 \n{4}{3}} \frac{1}{1-(n-C\n{2}{3})(n^{-1} + t\n{-4}{3})} \\
	&= \frac{n-C\n{2}{3}}{\delta^2 \n{4}{3}} \frac{\n{1}{3}}{C-t+Ct\n{-1}{3}} \\
	&\leq \frac{n}{\delta^2 \n{4}{3}} \frac{\n{1}{3}}{C-t+Ct\n{-1}{3}} \\
	&= \delta^{-2} \frac{1}{C-t+Ct\n{-1}{3}}.
	\end{aligned}
	\end{equation}
	For fixed $C$, $\delta$ and $t$
	this expression converges asymptotically to $\delta^{-2} \frac{1}{C-t}$ as $n \rightarrow \infty$.
	Therefore for all $\epsilon>0$ we can choose $C>0$ such that
	\begin{equation}
		\Prob(\ExcursionEvent) \leq \delta^{-2} \frac{1}{C-t+Ct\n{-1}{3}} < \epsilon.
	\end{equation}
	This completes the proof.
\end{proof}

We have shown that with high probability there is no mass of large excursions wandering off to infinity.
\fxnote{Correct use of "with high probability"?}
\fxnote{"Mass of large excursions" needs clarification.}
For all $\epsilon>0$ we can find a $C>0$ such that,
with probability $1-\epsilon$,
all excursions of size equal to or larger than $\delta\n{2}{3}$ will start before step $C\n{2}{3}$.
For all these excursions we can apply Lemma~\ref{L: Lemma 8} to prove that $\Xin \rightarrow_d \Xi$.
With these considerations we finish the proof of Theorem~\ref{T: Main}.

\section{Graph-theory approach}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: Graph-theory approach
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section, we present an  alternative proof of Lemma~\ref{L: late excursions},
which relies not on stochastic calculus but uses theory on random graphs.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Lemma 9 - Graph Components: Statement
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lemma} \label{L: Lemma 9}
	Let $p(n, y, \delta)$ be the chance that $\Gnt$
	contains a component of size greater than or equal to $\delta \n{2}{3}$
	which does not contain any vertex $i$ with $1 \leq i \leq y\n{1}{3}$.
	
	Then
	\begin{equation}
	\lim_{y \rightarrow \infty} \limsup_n p(n, y, \delta) = 0 \enspace
	\end{equation}
	for all $\delta > 0$.
\end{lemma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Lemma 9 - Graph Components: Statement
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
	For this proof we have to change our understanding of the random graph process $\Gnt$ a little.
	Until now, we constructed a random graph by labeling all vertices $\{1, \dots, n\}$ 
	and then drawing Bernoulli random variables to construct edges and consequently components.
	Now we switch the order of actions. 
	Start with an unlabeled empty graph with $n$ vertices.
	Draw $\Bern(p)$ random variables to establish edges between vertices.
	Then, one by one, assign the labels $1$ to $n$ to the vertices,
	where the probabilities are equal for all remaining vertices.
	\fxnote{Fix this sentence.}
	Since the construction of edges is independent of vertex labels and edge are constructed independently of each other,
	these two approaches are equivalent construction of the random graph.
	\fxnote{Construction is used twice here with different meaning.}
	
	Let $\SigmaAlgebra_E$ be the sigma-algebra that includes the creation of edges and components, 
	but not the labeling of the vertices.
	\fxnote{Alter the description of the sigma-algebra.}
	For a component $\Ccal$ of size $\alpha\n{2}{3}$,
	let $v(\Ccal)$ be the label of its minimal vertex and write 
	$\ChiAn = \n{-1}{3}v(\Ccal)$.
	We show that $\ChiAn \rightarrow_d \Exponential(\alpha)$.
	\fxfatal{Proof of Chi -> Exp goes here!}
	
	Fix $\delta > 0$.
	Let $\NComp{I}$ be the expected number of components of size greater than or equal to $\delta\n{2}{3}$ 
	with its minimal vertex label in $\n{1}{3}I$.
	Then
	\begin{equation} \label{E: q(n,[y,infty])}
	\begin{aligned}
	\NComp{[y, \infty)}
	&= \ExpBig{ \ExpBig{ \sum_{\Ccal: \; |\Ccal| \geq \delta\n{2}{3}} \Ind{\{ v(\Ccal) > y\n{1}{3} \}} \cond \SigmaAlgebra_E } } \\
	&= \ExpBig{ \sum_{\Ccal: \; |\Ccal| \geq \delta\n{2}{3}} \Prob( v(\Ccal) > y\n{1}{3} \cond \SigmaAlgebra_E) } \\
	&= \ExpBig{ \sum_{\Ccal: \; |\Ccal| \geq \delta\n{2}{3}} \Prob(\ChiAn > y \cond \SigmaAlgebra_E, |\Ccal| = \alpha\n{2}{3}) }
	\end{aligned}
	\end{equation}
	\fxfatal{Fix usage of P for probability. Maybe use mathbb(P).}
	Since the distribution of $\ChiAn$ converges to the exponential distribution,
	$\Prob(\ChiAn > y) \leq e^{-\alpha y} (1+ \Smallo{1})$,
	where the last factor describes an error from the convergence.
	\fxnote{Describes an error from the convergence... Fix once convergence is proven and understood.}
	Additionally, $\Prob(\ChiAn \leq 1) \leq (1 - e^{-\alpha}) (1+ \Smallo{1})$,
	so
	\begin{equation}
	\Prob(\ChiAn > y) \leq e^{-\alpha y} (1+ \Smallo{1}) = \frac{e^{-\alpha y}}{1 - e^{-\alpha}} \Prob(\ChiAn \leq 1) (1 + \Smallo{1}).
	\end{equation}
	Using this inequality in \eqref{E: q(n,[y,infty])} leads to
	\begin{equation}
	\begin{aligned}
	\NComp{[y, \infty)}
	&\leq \ExpBig{ \sum_{\Ccal: \; |\Ccal| \geq \delta\n{2}{3}}
		\frac{e^{-|\Ccal|\n{-2}{3}y}}{1-e^{-|\Ccal|\n{-2}{3}}} \Prob(\ChiAn \leq 1) (1 + \Smallo{1}) } \\
	&\leq \sup_{\alpha \geq \delta} \frac{e^{-\alpha y}}{1-e^{-\alpha}} (1 + \Smallo{1}) 
		\ExpBig{ \sum_{\Ccal: \; |\Ccal| \geq \delta\n{2}{3}} \Prob(\ChiAn \leq 1 \cond \SigmaAlgebra_E ) } \\
	&= \frac{e^{-\delta y}}{1-e^{-\delta}} (1 + \Smallo{1}) \NComp{[0,1]},
	\end{aligned}
	\end{equation}
	Dividing by $\NComp{[0,1]}$ gives us
	\begin{equation}
	\limsup_n \frac{\NComp{[y,\infty]}}{\NComp{[0,1]}} \leq \frac{e^{-\delta y}}{1-e^{-\delta}} (1 + \Smallo{1}) \xrightarrow{y \rightarrow \infty} 0.
	\end{equation}
	For this convergence to hold,
	either $\NComp{[0,1]} \rightarrow \infty$ or $\NComp{[y,\infty]} \rightarrow 0$ as $y \rightarrow \infty$.
	Since the law of total expectation implies $p(n,y,\delta) \leq \NComp{[y,\infty]}$
	we only need to prove that
	\begin{equation}
	\sup_n \NComp{[0,1]} < \infty.
	\end{equation}
	Consulting existing literature on random graphs gives us $\sup_n \NComp{[0, \infty]} < \infty$.
	Because $\NComp{I_1} \leq \NComp{I_2}$ for $I_1 \subset I_2$,
	this concludes the proof.
	
	\fxfatal{Add reference to literature and proof of claim.}
\end{proof}
