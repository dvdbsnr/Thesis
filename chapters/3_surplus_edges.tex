% Chapter 3: Surplus edges
% Contains:
%   Definition of surplus edges
%   Eligible edges for s.e.
%   Calculating the probability of s.e.
%   The proof of (Z_n, N_n) => (W,N)
%   Why the overestimated probability is OK

\chapter{Surplus edges}
\fxnote{Update title.}

Theorem~\ref{T: Main} deals with surplus edges.
The goal of this chapter will be to first define the notion of surplus edges,
find a way to estimate the probability of encountering one and finally proving the joint convergence of $Z_n$ and the other process.

\section{Counting surplus edges}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: Counting surplus edges
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\fxnote{Update title.}

We begin by describing a way to analyse the appearance of surplus edges.
In Chapter~\ref{C: bf-walk} we defined the breadth-first walk $Z_n$, 
which counted new connections to previously disconnected vertices.
We remind ourselves that a surplus edge in a graph $\Gcal \in \Gnt$ appears if,
during the transition of vertices and components of the breadth-first walk,
a vertex opens a new connection to another vertex, 
which already has opened connections to some explored node.
We associate with $\Gcal$ a counting process $(\nnt(s), 0 \leq s \leq n)$,
with $\nnt(0) = 0$, which increases by $1$ at each appearance of a surplus edge.

In fact $\nnt$ is a process very similar to $B_n$,
the process defined in Chapter~\ref{C: bf-walk} that counted each appearance of a new edge to an ineligible vertex.
Lemma~\ref{L: formula an} established that $\Bn{t} = \int_{0}^{t} \an{s}ds$ with
\begin{equation*}
	\an{s} = (n - \Ineligible{s}) \ps,
\end{equation*}
where $\Ineligible{s}$ is the number of vertices ineligible to become a child of $v(\ceil{s})$ at time $s$.
In terms of counting processes, we call $a_n$ the rate or conditional intensity of $B_n$.
It is evident that $\nnt$ will have a similar rate,
substituting the number of vertices eligible to become a child of $v(\ceil{s})$ with the number of vertices eligible to receive a surplus edge to $v(\ceil{s})$.

\begin{figure}[ht]
	\input{figures/fig_tree1.tex}
	\caption{A tree} 
	\label{F: Surplus Edges Tree}
\end{figure}

Consider the breadth-first walk on the graph of Figure~\ref{F: Surplus Edges Tree} at time $2$.
The children of $v_1$, $v_2$ to $v_4$, and the children of $v_2$, that is $v_5$ and $v_6$, are already discovered.
We are interested in surplus edges to $v_3$.
So $v_1$ to $v_6$ are unable to form edges to become children of $v_3$ and $\Ineligible{2} = |\{ v_1, \dots,  v_6\}|$.
Of these vertices, $v_1$ and $v_2$ are already explored an every connection to other nodes is known.
Vertex $v_3$ can not have an edge to itself, so only $v_4$, $v_5$ and $v_6$ are eligible to receive a surplus edge to $v_3$.

In general, at time $i-1$, the first $i$ vertices are ineligible for a surplus edge to $v(i)$.
The remaining $\Ineligible{i-1} - i$ vertices are candidates for an excess edge opening with probability $\p$.
Therefore, the counting process $\nnt$ has rate
\begin{equation} \label{E: rate Nnt}
(\Ineligible{\floor{s}} - \floor{s}) \ps.
\end{equation}
Note that this rate is only exact for the chance of encountering exactly one surplus edge, 
but an overestimation for multiple excess edges.
If we encounter a surplus edge at some time $s \in [i-1, i)$,
the number of eligible vertices should decrease by one.
However, \eqref{E: rate Nnt} is constant for all $s \in [i-1, i)$.
For ease of computation we will continue with this overestimation and later argue that the difference becomes negligible as $n \rightarrow \infty$.

Lemma~\ref{L: formula an} established $\Ineligible{s} = s + \Zetan{\ceil{s}} + \Zn{s}$ 
and using \eqref{E: zeta-1(i) = 1 - min(j)} we can rewrite
\begin{equation*}
\begin{aligned}
\Ineligible{\floor{s}} - \floor{s} 
&= \floor{s} - \ZetaMinus{\floor{s}+1} + \znt(\floor{s}) - \floor{s} \\
&= 1 - \min_{u \leq \floor{s}} \znt(u) + \znt(\floor{s}),
\end{aligned}
\end{equation*}
\fxfatal{Where does the 1 go?}
and the conditional intensity becomes
\begin{equation} \label{E: rate Nnt 2}
(1 - \min_{u \leq \floor{s}} \znt(u) + \znt(\floor{s}) \ps.
\end{equation}
We now rescale the counting process via
\begin{equation} \label{E: rescale Nnt}
\rnnt(s) = \nnt(\n{2}{3}s).
\end{equation}
We calculate the rate of this rescaled process.
Recall that the conditional intensity $\bar{\lambda}(s)$ of the process $\rnnt(s)$ must satisfy
\begin{equation}
\Exp{\rnnt(s)} = \int_{0}^{s} \lambda(u) du.
\end{equation}
Denote the rate of $\nnt$ by $\lambda(s)$ and refer to \eqref{E: rescale Nnt} to evaluate
\begin{equation*}
\begin{aligned}
\Exp{\rnnt(s)}
&= \Exp{\nnt(\n{2}{3}s)} \\
&= \int_{0}^{\n{2}{3}s} \lambda(u) du \\
&= \int_{0}^{s} \n{2}{3} \lambda(\n{2}{3}u) du.
\end{aligned}
\end{equation*}
Comparing both rates directly gives us
\begin{equation*}
\begin{aligned}
\bar{\lambda}(s)
&= \n{2}{3} \lambda(\n{2}{3}s) \\
&= \n{2}{3} \frac{1 - \min_{u \leq \n{2}{3}s} \znt(u) + \znt(\n{2}{3}s)}{1 - (\n{2}{3}s - \floor{\n{2}{3}s})\p} \p \\
&= \n{2}{3} \frac{1 - \n{1}{3} \min_{u \leq s} \rznt(u) + \n{1}{3}\rznt(s)}{1 - (\n{2}{3}s - \floor{\n{2}{3}s})\p} \p \\
&= \underbrace{n \p}_{\rightarrow 1} \frac{\n{-1}{3} - \min_{u \leq s} \rznt(u) + \rznt(s)}{1 - (\n{2}{3}s - \floor{\n{2}{3}s})\p} \\
&\xrightarrow{n \rightarrow \infty} \rznt(s) - \min_{u \leq s}\rznt(u).
\end{aligned}
\end{equation*}

By Theorem~\ref{T: Z -> W} we have $\rznt \rightarrow_d \Wt$,
so
\begin{equation}
\rznt(s) - \min_{u \leq s}\rznt(u) \rightarrow_d \Wt(s) - \min_{u \leq s}\Wt(u) = \Bt.
\end{equation}


The rate of the counting process $\Nt$ therefore converges to $\Bt$.
\fxfatal{Find out what's the problem with the joint convergence.}


\section{On weak convergence}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: On weak convergence
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\fxnote{Introductory text.}

\fxnote{Definition weak convergence}
This section will probably move to a preliminary chapter.
\begin{definition}[Weak convergence]
	Let $S$ be a metric space and $\mathcal{S}$ it's class of Borel sets.
	Let $P, P_1, P_2, \dots$ be probability measures on defined on $S$.
	We say $P_n$ converges weakly to $P$, $P_n \Rightarrow P$, if
	\begin{equation} 
	\int_S fdP_n \xrightarrow{n \rightarrow \infty} \int_S fdP
	\end{equation}
	holds for all continuous, bounded functions $f: S \rightarrow \Real$.
\end{definition}

This definition now applies to convergence in distribution of random variables as follows:
Let $X, X_1, X_2, \dots$ be random variables on some probability space $S$.
Let $P, P_1, P_2, \dots$ be the corresponding distributions. 

\begin{definition}[Tightness] \label{D: Tightness}
	Inhalt...
\end{definition}

\begin{definition}[Relative compactness] \label{D: Rel Compactness}
	Inhalt...
\end{definition}

\begin{theorem}[Prohorov's theorem] \label{T: Prohorov}
	Let $P_n$ be a series of probability measures on some metric space $S$. If $P_n$ is tight, then it is relatively compact.
\end{theorem}



We will prove weak convergence by showing that
\begin{equation}
\Exp{ f(\bar{Z}^t_n, \bar{N}_n) } \longrightarrow \Exp{ f(W^t, N) }
\end{equation}
as $n \rightarrow \infty$, 
for all continuous, bounded functions 
$f: D^2 \rightarrow \Real$.


\section{Weak convergence of $(Z^t_n, N^t_n)$}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECTION: Weak convergence of $(Z^t_n, N^t_n)$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We begin this chapter by an introduction on what the rate of a counting process means.
This includes those pictures in my notes on the height of the excursion and stuff.
We define $B_n$ to be the "height" of the exploration in $\Gnt$, aka the discrete version of $\Bt$.
We now state the main Theorem of this chapter.
\begin{theorem} \label{T: Joint Convergence}
	For the previously defined processes $\rznt$ and $\rnnt$,
	the joint weak convergence
	\begin{equation}
	( \rznt(s), \rnnt(s); s \geq 0 ) \longrightarrow_d (\Wt(s), \Nt(s); s \geq 0)
	\end{equation}
	holds, where $\Nt$ is the counting process with conditional intensity $\Bt$,
	i.e. the process for which
	\begin{equation*}
	\Nt(s) - \int_{0}^s \Bt(u)du
	\end{equation*}
	is a martingale.
\end{theorem}
\begin{proof}

For $n \in \Nat$ consider the process $\nnt$.
The increase of $\nnt$ in between times $i-1$ and $i$ will be $\Binom(\Bn{i-1},\p)$-distributed.
Define $\rnnt(s) = \nnt(\n{2}{3}s)$.

We want to show that
\begin{equation}
(\rznt, \rnnt) \rightarrow_d (\Wt, \Nt),
\end{equation}
meaning 
\begin{equation}
\Exp{f(\rznt, \rnnt)} \xrightarrow{n \rightarrow \infty} \Exp{f(\Wt, \Nt)}
\end{equation}
for all continuous, bounded functions $f:D[0,T]^2\rightarrow\Real$.

We already know that $\rznt \rightarrow_d \Wt$, which implies two conditions.
First, $\rznt$ is tight, 
so for all $\epsilon>0$ there exists a compact $K \subset D[0,T]$ such that
\begin{equation}
\inf_n \Prob ( \rznt \in K) > 1 - \epsilon.
\end{equation}

Second, $\rznt$ converges in finite dimensional distributions.
So for all $s_1 < s_2 < \dots < s_k$:
\begin{equation}
(\rznt(s_1), \dots, \rznt(s_k)) \rightarrow_d (\Wt(s_1), \dots, \Wt(s_k)).
\end{equation}

We can use the Skorohod representation theorem to define random variables $X$, $X_1$, $X_2$, \dots
on the same probability space, such that $X \sim \Wt$, $X_i \sim \bar{Z}^t_i$ for all $i$
and $X_n \rightarrow_{text{a.s.}} X$.
For random variables mapping to functions, we have
\begin{equation}
\sup_{s \in \Real_{\geq 0}} |X_n(s) - X(s)| \rightarrow_{text{a.s.}} 0.
\end{equation}
So
\begin{equation} 
\sup_{s \in \Real_{\geq 0}} |X_n(s) - X(s)| \rightarrow_p 0,
\end{equation}
meaning for all $\epsilon > 0$:
\begin{equation}
\Prob( \sup_{s \in \Real_{\geq 0}} |X_n(s) - X(s)| > \epsilon ) \rightarrow 0.
\end{equation}
From now on, writing $\rznt$ and $\Wt$ we mean these random variables.

By the reflection principle, for all $\epsilon>0$ exists an $A>0$ such that
\begin{equation}
\Prob( \sup_{s\leq T} |\Wt(s)| > A ) \leq \Prob( \sup_{s\leq T} \Wt(s) > A ) = 2 \Prob( \Wt(T) > A ) < \epsilon.
\end{equation}
This implies that for all $\epsilon > 0$ exists $A>0$ such that for all $n$
\begin{align}
\Prob(\sup_{s \leq T} |\rznt(s)| > A) &< \epsilon \\
\Prob(\sup_{s \leq T} |\rzminz(s)| > A) &< \epsilon,
\end{align}
where $\rzminz(s) = \rznt(s) - \min_{u \leq s}\rznt(u) \rightarrow_d \Bn{s}$.
\fxnote{Add definition of B, Bbar etc.}

To see this, fix $\epsilon > 0$ and chose $A>0$ such that $A - \sup_{s\leq T} |\Wt(s)| \geq \delta > 0$.
Let $N$ such that $\sup_{s\leq T} |\rznt(s) - \Wt(s)| < \delta$.
Choose $A^* = \max(N, A)$. Then either $\sup_{s\leq T} |\rznt(s) < A^*$ since $T \leq N$ or $\rznt$ is constant after $N$ or because of the convergence.
\fxfatal{This is a paceholder until I see if I really need this part.}

Therefore
\begin{equation}
\Prob( \sup_{s \leq T} |\zminz(\n{2}{3}s)| \geq A\n{1}{3} ) < \epsilon \forall n.
\end{equation}

Our next goal is showing that the sequence $\rnnt$ is tight.
That is, for all $\epsilon > 0$ exists $K>0$ such that
\begin{equation}
\Prob(\sup_{s \leq T} |\rnnt(s)| > K) = \Prob( \rnnt(T) > K ) < \epsilon.
\end{equation}
For $i \in [0, \n{2}{3}T]$, $\nnt(i) - \nnt(i-1) \sim \Binom(\zminz(i-1), \p)$.
Additionally $\zminz(i-1) \leq \sup_{j \leq \n{2}{3}T} \zminz(j) \leq \n{1}{3}A$ with probability greater than $1-\epsilon$.
Conditioning on the event that $\zminz(i-1) \leq A\n{1}{3}$,
\begin{equation}
X_i \sim \Binom(\zminz(i-1), \p) \leq_{\text{st.}} Y_i \sim \Binom(A\n{1}{3}, \p).
\end{equation}
Then
\begin{equation}
\nnt(T\n{2}{3}) \leq_{\text{st.}} \sum_{j=1}^{T\n{2}{3}} Y_j,
\end{equation}
where $Y_1, Y_2, \dots \sim \Binom(A\n{1}{3}, \p)$ i.i.d.

Therefore
\begin{align*}
\Prob(\rnnt(T) \geq K) 
&= \Prob(\nnt(\n{2}{3}T) \geq K\n{1}{3}) \\
&= \Prob(\nnt(\n{2}{3}T) \geq K\n{1}{3}) \Prob(\zminz(i-1) > A\n{1}{3}) \\
&\quad + \Prob(\nnt(\n{2}{3}T) \geq K\n{1}{3}) \Prob(\zminz(i-1) \leq A\n{1}{3}) \\
&\leq \epsilon + \Prob( \sum_{j=1}^{T\n{2}{3}} Y_j \geq K) \\
&\leq \epsilon + \frac{1}{K}\n{2}{3}T \Exp{Y_1} \\
&= \epsilon + \frac{1}{K}\n{2}{3}T \p A\n{1}{3} \\
&= \epsilon + \frac{1}{K}n\p T \\
&\leq \epsilon + CAT\frac{1}{K} \qquad \text{for some $C>0$} \\
&\leq 2\epsilon
\end{align*}
\fxnote{Fix the use of > and >= here.}
\fxnote{Fix the use of A/K here.}
for large $K$.
Thus $\znt$ is tight.
\fxnote{What is tight? In n or s?}

Since $\nnt$ is monotonically increasing, $\rnnt$ is tight as a random variable with image in $D$.
This is a new fact, since we previously only knew that the rate of $\rnnt$ converges and is tight.
\fxnote{Show that this is true.}

Next, we will show that
\begin{equation}
\Exp{f(\Wt, \rnnt)} = \Exp{ \Exp{f(\Wt, \rnnt) \cond \Wt} } \rightarrow \Exp{f(\Wt, \Nt)} = \Exp{ \Exp{f(\Wt, \Nt) \cond \Wt} }.
\end{equation}
The function $f: D^2 \rightarrow \Real$ is bounded and continuous,
so $f(\Wt,\cdot): D \rightarrow \Real$ is bounded and continuous for all $\Wt \in D$.
So we will prove that for all trajectories $\Wt$, $\rnnt \rightarrow_d \Nt$.

Both $\znt$ and $\rnnt$ are tight for all $n$,
so $(\znt, \rnnt)$ is tight for all $n$.
So for all $\epsilon$ there exists a compact set $C \subset D^2$ such that
\begin{equation}
\Prob ((\znt, \rnnt) \in C) > 1 - \epsilon.
\end{equation}
The function $f$ is bounded by some real number $M$.
So even if $(\znt, \rnnt)$ is not in this compact set,
\begin{equation}
\Exp{f(\znt, \rnnt)} \leq M\epsilon + \Exp{f(\znt, \rnnt) \cond (\znt, \rnnt) \in C}.
\end{equation}
Remember the assumption that $\znt$ and $\Wt$ are defined on the same probability space,
where they converge almost surely.
Then for all $\epsilon, \delta > 0$, there exists $N_0$ sucht that for all $n > N_0$,  
$\Prob(||\znt - \Wt||>\delta) < \epsilon$.
Then
\begin{equation}
\Exp{f(\znt, \rnnt)} \leq 2M\epsilon \Exp{f(\znt, \rnnt) \cond (\znt, \rnnt) \in C, ||\znt - \Wt||>\delta}.
\end{equation}
Consider
\begin{equation*}
f(\znt, \rnnt) = f(\znt, \rnnt) + f(\Wt, \rnnt) - f(\Wt, \rnnt) \leq f(\Wt, \rnnt) + |f(\znt, \rnnt) - f(\Wt, \rnnt)|
\end{equation*}
so 
\begin{equation}
\begin{aligned}
\Exp{f(\znt, \rnnt)} 
&\leq 2M\epsilon \Exp{f(\Wt, \rnnt) + |f(\znt, \rnnt) - f(\Wt, \rnnt)| \cond (\znt, \rnnt) \in C, ||\znt - \Wt||>\delta } \\
&\leq 2M\epsilon + \epsilon + \Exp{f(\Wt, \rnnt), \cond (\znt, \rnnt) \in C, ||\znt - \Wt||>\delta}
\end{aligned}
\end{equation}
by the continuity of $f$.

We define a process $\Ndis$, which may be thought of as $\Nt$ in discrete time.
While $\Nt$ is the continuous counting process using $\Bt$ as rate,
we define $\Ndis$ by
\begin{equation}
\begin{aligned}
\Ndis(0) &:= 0, \\
\Ndis(k) &:= \Ndis(k-1) + \xi_k, 
\end{aligned}
\end{equation}
where $\xi_k \sim \Binom(\n{1}{3}\Bt(\n{2}{3}k), \p)$.
Now define $\rNdis(k) := \n{-1}{3} \Ndis(\n{2}{3}k)$.

We will show that, if $||\znt - \Wt|| < \delta$, 
then $\Ndis(k) = \nnt(k)$ for all $k \leq \n{2}{3}T$ with hight probability.
For this, we use a coupling argument to redefine both processes.
\fxnote{Find out what a coupling argument is.}
At step $k$, let
\begin{equation}
\begin{aligned}
\alpha_k := \min(\zminz(k), \n{1}{3}\Bt(\n{-2}{3}k)), \\
\beta_k := \max(\zminz(k), \n{1}{3}\Bt(\n{-2}{3}k)).
\end{aligned}
\end{equation}
Note that instead of rescaling $\zminz$ to fit $\Bt$, we rescaled $\Bt$.
Now define random variables
\begin{equation}
\begin{aligned}
\xi_k \sim \Binom(\alpha_k, \p),
\eta_k \sim \Binom(\beta_k - \alpha_k, \p).
\end{aligned}
\end{equation}
So $\xi_k + \eta_k \sim \Binom(\beta_k, \p)$.

Consider the two possibilities at time $k$:
Either $\zminz(k) \leq \n{1}{3}\Bt(\n{-2}{3}k)$,
then $\alpha_k = \zminz(k)$ and $\beta_k = \n{1}{3}\Bt(\n{-2}{3}k)$, 
so $\xi_k =_d \nnt(k) - \nnt(k-1)$ and $\xi_k + \eta_k =_d \Ndis(k) - \Ndis(k)$.
Or $\zminz(k) > \n{1}{3}\Bt(\n{-2}{3}k)$,
then $\alpha_k = \n{1}{3}\Bt(\n{-2}{3}k)$ and $\beta_k = \zminz(k)$, 
so $\xi_k =_d \Ndis(k) - \Ndis(k)$ and $\xi_k + \eta_k =_d \nnt(k) - \nnt(k-1)$.

This way, we can define $\nnt$ and $\Ndis$ by
\begin{equation*}
\nnt(k) - \nnt(k-1) = 
\begin{cases}
\xi_k & \zminz(k) \leq \n{1}{3}\Bt(\n{-2}{3}k) \\
\xi_k + \eta_k &\text{else}
\end{cases}
\end{equation*}
and
\begin{equation*}
\Ndis(k) - \Ndis(k-1) = 
\begin{cases}
\xi_k + \eta_k & \zminz(k) \leq \n{1}{3}\Bt(\n{-2}{3}k) \\
\xi_k &\text{else}.
\end{cases}
\end{equation*}
By the definitions of $\xi_k$ and $\eta_k$,
these definitions maintain
\begin{equation}
\begin{aligned}
\nnt(k) - \nnt(k-1) &\sim \Binom(\zminz(k), \p), \\
\Ndis(k) - \Ndis(k-1) &\sim \Binom(\n{1}{3} \Bt(\n{-2}{3}k), \p).
\end{aligned}
\end{equation}

Conditioning on $||\znt - \Wt|| < \delta$, which we call $\Event{\delta}$,
for $\Ndis(k) \neq \nnt(k)$ for some $k$,
there has to have been step in which the increments of both processes have differed.

\begin{align*}
&\Prob(\exists k\leq \n{2}{3}T: \Ndis(k) \neq \nnt(k) \cond \Event{\delta}) \\
&\quad \leq \sum_{k=1}^{\n{2}{3}T} \Prob( \Ndis(k) - \Ndis(k-1) \neq \nnt(k) - \nnt(k-1) \cond \Event{\delta}) \\
&\quad \leq \n{2}{3}T \max_{k \leq \n{2}{3}T} \Prob(\Ndis(k) - \Ndis(k-1) \neq \nnt(k) - \nnt(k-1) \cond \Event{\delta}) \\
&\quad = \n{2}{3}T \max_{k \leq \n{2}{3}T} \Prob( \eta_k \neq 0 \cond \Event{\delta} ).
\end{align*}

Since, by $||\znt - \Wt|| < \delta$, we know $\beta_k - \alpha_k < \delta \n{1}{3}$,
$\eta_k \leq_{\text{st.}} \zeta_k \sim \Binom(\delta\n{1}{3}, \p)$.

So 
\begin{equation}
\Prob( \eta_k \neq 0 \cond \Event{\delta} ) \leq \Prob( \zeta_k \neq 0) = \Prob( \zeta_k \geq 1 ) \leq \Exp{\zeta_k} = \delta\n{1}{3}\p.
\end{equation}

So
\begin{align*}
&\Prob(\exists k\leq \n{2}{3}T: \Ndis(k) \neq \nnt(k) \cond \Event{\delta}) \\
&\quad \leq \n{2}{3}T\delta\n{1}{3}\p \\
&\quad \leq n \p T \delta \\
&\quad \leq 2\delta 
\end{align*}
for large $n$.

Therefore
\begin{align*}
\Exp{f(\znt, \nnt)} 
&\leq 2M\epsilon + \epsilon + \Exp{f(\Wt, \nnt) \cond \Event{C}, \Event{\delta}} \\
&\leq 2M\epsilon + \epsilon + 2\delta T M + \Exp{f(\Wt, \Ndis) \cond \Event{C}, \Event{\delta}}.
\end{align*}
\fxnote{Epsilon here?}

Since $\nnt$ is tight  and $\nnt$ is equal to $\Ndis$ with hight probability,
$\Ndis$ is tight.

We know show that $\rNdis \rightarrow_d \Nt$.
It suffices proving convergence in finite dimensional distributions,
in this case for all $s_1 < s_2 < \dots < s_l \leq T$:
\begin{equation}
	\Prob( \rNdis(s_1) = k_1, \dots, \rNdis(s_l) = k_l ) \rightarrow 
	\Prob( \Nt(s_1) = k_1, \dots, \Nt(s_l) = k_l )
\end{equation}
as convergence in $\Real$.

Remember that we are still working with a fix trajectory $\Wt$.
Recall $\Nt$ is a poisson point process (p.p.p.), continuous on $\Real$ with rate $\Bt$,
and $\Ndis$ is a discrete process whose increments are defined by $\Bt$ at integer times.
By definition of a p.p.p., the increments of $\Nt$ are independent and
for all $a<b$: $\Nt(b) - \Nt(a) \sim \Poisson( \int_{a}^{b} \Bt(s)ds)$.
Similarly, for all $k$: $\Ndis(k) - \Ndis(k-1) \sim \Binom(\Bt(\n{2}{3}k), p)$.

Then
\begin{align*}
&\Prob(\rNdis(s_1) = k_1, \dots, \rNdis(s_l) = k_l) \\
&\quad = \Prob( \rNdis(s_j) - \rNdis(s_{j-1}) = k_j - k_{j-1} \enspace \forall j=2,\dots,l ) \\
&\quad = \prod_{j=2}^{l} \Prob( \n{1}{3}\rNdis(s_j) - \n{1}{3}\rNdis(s_{j-1}) = \n{1}{3}(k_j - k_{j-1})) \\
&\quad = \prod_{j=2}^{l} \Prob( Y_j = \n{1}{3}(k_j - k_{j-1}) ),
\end{align*}
where
\begin{equation}
Y_j \sim \Binom(\underbrace{\sum_{i=\n{2}{3}s_{j-1}+1}^{\n{2}{3}s_j}\n{1}{3}\Bt(\n{-2}{3}i)}_{=: R_{n,j}}, \p).
\end{equation}
Note that $Y_j =_d \sum_{k=1}^{R_j} \xi_k$, where $\xi_k \sim \Bern(\p)$.

Now
\begin{align*}
\Exp{\sum_{k=1}^{R_j} \xi_k} 
&= R_j \p \\
&\approx \frac{1}{n} R_j \\
&= \frac{1}{n} \sum_{i=\n{2}{3}s_{j-1}+1}^{\n{2}{3}s_j}\n{1}{3}\Bt(\n{-2}{3}i) \\
&= \sum_{i=\n{2}{3}s_{j-1}+1}^{\n{2}{3}s_j}\n{-2}{3}\Bt(\n{-2}{3}i) \\
&\xrightarrow{\text{Riemann Sum}} \int_{s_{j-1}}^{s_j} \Bt(u)du.
\end{align*}

We state the Poisson Convergence Theorem:
Let $Y_k \sim \Binom(k, p_k)$ and let $\Exp{Y_k} = kp_k \rightarrow \lambda \in \Real$.
Then $Y_k \rightarrow_d \Poisson(\lambda)$.

Applied here:
\begin{equation}
Y_j =_d \sum_{k=1}^{R_j} \xi_k \rightarrow_d \Poisson( \int_{s_{j-1}}^{s_j} \Bt(u)du ) =_d \Nt(s_j) - \Nt(s_{j-1}).
\end{equation}
So $\Ndis \rightarrow \Nt$ and
\begin{equation}
	\Exp{f(\Wt, \Ndis) \cond \Wt} \rightarrow \Exp{f(\Wt, \Nt) \cond \Wt}.
\end{equation}
Therefore
\begin{equation}
	\Exp{f(\znt, \nnt)} = \Exp{\Exp{f(\znt, \nnt) \cond \Wt}} \rightarrow \Exp{\Exp{f(\Wt, \Nt) \cond \Wt}} = \Exp{f(\Wt, \Nt)},
\end{equation}
so $(\znt, \nnt) \rightarrow_d (\Wt, \Nt)$.

















\end{proof}

\newpage
We can now assure ourselves that the overestimated probability \eqref{E: rate Nnt} is asymptotically negligible.
Assume the chance that any vertex encounters two or more surplus edges is non-zero and does not converge to zero as $n \rightarrow \infty$.
If a vertex connects by multiple excess edges, the process $\nnt$ makes two or more jumps during the time-interval of length $1$.
The rescaling \eqref{E: rescale Nnt} compresses the time axis until, in the limit process $\Nt$, 
any distance in an interval of original length $1$ will be reduced to a single point.
Therefore there would be a non-zero chance that the counting process has multiple coincident points.
By general theory on counting processes, this is not possible.
\fxnote{Find quote for theory on counting processes.}
So the probability of a vertex having multiple surplus edges must tend to zero.

















